{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d6bd4e8-1316-4cd1-9dd2-1e9e381ac245",
   "metadata": {},
   "source": [
    "# LSTM Volatility Forecasting with Hyperparameter Optimization\n",
    "\n",
    "## Overview\n",
    "This notebook builds and optimizes a Long Short-Term Memory (LSTM) neural network to predict 10-day ahead volatility in financial markets. The project includes:\n",
    "\n",
    "- **Data preparation**: Feature engineering with lagged volatility values\n",
    "- **Hyperparameter tuning**: Automated search using Keras Tuner's Hyperband algorithm to find optimal model architecture, regularization, and training parameters\n",
    "- **Model evaluation**: Comprehensive performance analysis including:\n",
    "  - Regression metrics (R², MSE, MAE)\n",
    "  - Directional accuracy (predicting volatility increases/decreases)\n",
    "  - Visual diagnostics across 12+ detailed plots\n",
    "- **Comparison**: Benchmarking the optimized model against a baseline LSTM\n",
    "\n",
    "The goal is to forecast future volatility levels with high accuracy while understanding the model's ability to predict directional changes—critical for risk management and trading applications.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ad2d29-4ade-4597-a5d8-140218cbc966",
   "metadata": {},
   "source": [
    "## Dependencies\n",
    "\n",
    "Core libraries for data processing, machine learning, and visualization:\n",
    "- **pandas/numpy**: Data manipulation and numerical operations\n",
    "- **scikit-learn**: Data preprocessing (StandardScaler), baseline models (LinearRegression), and evaluation metrics\n",
    "- **matplotlib/seaborn**: Visualization and plotting\n",
    "- **tensorflow/keras**: Deep learning framework for building and training LSTM models\n",
    "  - Sequential: Model architecture\n",
    "  - LSTM, Dense, Dropout, BatchNormalization: Neural network layers\n",
    "  - EarlyStopping: Training callbacks to prevent overfitting\n",
    "  - regularizers: L1/L2 regularization techniques\n",
    "- **keras-tuner**: Automated hyperparameter optimization using Hyperband algorithm\n",
    "- **scipy.stats**: Statistical analysis and distribution testing\n",
    "- **os/pathlib**: File system operations and path management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe350561-04ba-4818-9cfd-c30a8d611a0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-19 09:38:10.473980: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-11-19 09:38:10.475053: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-11-19 09:38:10.492456: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-11-19 09:38:10.492471: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-11-19 09:38:10.492979: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-11-19 09:38:10.496235: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-11-19 09:38:10.496732: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-11-19 09:38:10.903086: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend\n"
     ]
    }
   ],
   "source": [
    "### Dependencies\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error, confusion_matrix, classification_report, accuracy_score\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras import regularizers\n",
    "import keras_tuner as kt\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0b9e89f4-845e-4aff-befb-6a541960d089",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Configure TF to use 16 threads of my CPU (optional)\n",
    "#tf.config.threading.set_intra_op_parallelism_threads(16)\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "#def set_seeds(seed=42):\n",
    "#    np.random.seed(seed)\n",
    "#    tf.random.set_seed(seed)\n",
    "#    random.seed(seed)\n",
    "#    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "#    \n",
    "#    # Make TensorFlow operations deterministic\n",
    "#    os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
    "#    os.environ['TF_CUDNN_DETERMINISTIC'] = '1'\n",
    "#    \n",
    "#    # Configure TensorFlow for reproducibility\n",
    "#    tf.config.experimental.enable_op_determinism()\n",
    "#\n",
    "## Call at the start\n",
    "#set_seeds(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33810be0-4dec-458e-8f48-a0b8510bb82c",
   "metadata": {},
   "source": [
    "## Data Loading and Feature Engineering\n",
    "\n",
    "### Purpose\n",
    "Load historical S&P 500 data and engineer features suitable for time-series volatility forecasting with LSTM models.\n",
    "\n",
    "### Key Functions\n",
    "\n",
    "**`load_csv()`**\n",
    "- Loads CSV data from the `data/` directory\n",
    "- Parses dates in MM/DD/YY format\n",
    "- Corrects century ambiguity (e.g., '30' → 1930, not 2030)\n",
    "- Sorts chronologically for time-series analysis\n",
    "\n",
    "**`add_features()`**\n",
    "- **Returns & Volatility**: Daily returns and rolling 10-day volatility (our target metric)\n",
    "- **Price Features**: Intraday ranges, close-to-open changes, moving averages (5-day, 10-day)\n",
    "- **Volume Features**: Volume percent change and 5-day moving average\n",
    "- **Lag Features**: Previous day returns and volatility for temporal context\n",
    "- **Target Variable**: 10-day ahead volatility (`Volatility_future`)\n",
    "- **Data Cleaning**: \n",
    "  - Replaces infinite values with NaN\n",
    "  - Fills missing volume data\n",
    "  - Removes rows with missing key features\n",
    "- **Standardization**: Scales all features using StandardScaler for optimal neural network training\n",
    "\n",
    "### Data Split\n",
    "- **80% training set**: Used for model training and hyperparameter tuning (with internal validation split)\n",
    "- **20% test set**: Held out for final evaluation, maintains temporal order to simulate real-world forecasting\n",
    "\n",
    "This approach ensures the model learns from historical patterns while being evaluated on unseen future data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb460460-ab85-4639-8176-f9175d1cbf8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1097440/1616189166.py:11: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df = pd.read_csv(data_path, parse_dates=['Date'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Return  High_Low_pct  Close_Open_pct       MA5      MA10  \\\n",
      "1  0.023311      -0.92304       -0.023226 -0.647124 -0.647122   \n",
      "2 -0.217569      -0.92304       -0.023226 -0.647148 -0.647185   \n",
      "3  0.554534      -0.92304       -0.023226 -0.647173 -0.647209   \n",
      "4  0.454640      -0.92304       -0.023226 -0.647201 -0.647234   \n",
      "5  0.737950      -0.92304       -0.023226 -0.647104 -0.647214   \n",
      "\n",
      "   Volume_pct_change  Volume_MA5  Return_lag1  Return_lag2  Volatility_lag1  \n",
      "1          -0.060194   -0.522899    -1.399673     0.499832        -0.166255  \n",
      "2          -0.060194   -0.522899     0.023281    -1.399676        -0.217933  \n",
      "3          -0.060194   -0.522899    -0.217598     0.023273        -0.217969  \n",
      "4          -0.060194   -0.522899     0.554501    -0.217605        -0.203854  \n",
      "5          -0.060194   -0.522899     0.454607     0.554491        -0.210628   1    0.007275\n",
      "2    0.007273\n",
      "3    0.007181\n",
      "4    0.008303\n",
      "5    0.007715\n",
      "Name: Volatility_future, dtype: float64\n",
      "Train size: 18642, Test size: 4660\n"
     ]
    }
   ],
   "source": [
    "### Data Loading\n",
    "\n",
    "def load_csv(file_name: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load a CSV file from the data directory.\n",
    "    \"\"\"\n",
    "    data_path = Path.cwd() / \"data\" / file_name \n",
    "    if not data_path.exists():\n",
    "        raise FileNotFoundError(f\"{data_path} not found.\")\n",
    "\n",
    "    df = pd.read_csv(data_path, parse_dates=['Date'])\n",
    "    \n",
    "    # Parse 'Date' column explicitly\n",
    "    # MM/DD/YY format\n",
    "    df['Date'] = pd.to_datetime(df['Date'], format='%m/%d/%y', errors='coerce')\n",
    "    \n",
    "    # Correct future years (pandas may interpret '30' as 2030)\n",
    "    # Assume dates from 1928–2020\n",
    "    df.loc[df['Date'] > pd.Timestamp.today(), 'Date'] -= pd.offsets.DateOffset(years=100)\n",
    "    \n",
    "    # Sort by date\n",
    "    df = df.sort_values('Date').reset_index(drop=True)\n",
    "    return df \n",
    "\n",
    "\n",
    "\n",
    "def add_features(df: pd.DataFrame, rolling_window: int = 10):\n",
    "    \"\"\"\n",
    "    Add features for ML: returns, rolling volatility, price/volume features, lags.\n",
    "    Handles NaN and infinite values, ready for scaling.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Returns and volatility\n",
    "    df['Return'] = df['Adj Close'].pct_change()\n",
    "    df['Volatility'] = df['Return'].rolling(rolling_window).std()\n",
    "\n",
    "    # Price-based features\n",
    "    df['High_Low_pct'] = (df['High'] - df['Low']) / df['Close']\n",
    "    df['Close_Open_pct'] = (df['Close'] - df['Open']) / df['Open']\n",
    "    df['MA5'] = df['Close'].rolling(5).mean()\n",
    "    df['MA10'] = df['Close'].rolling(10).mean()\n",
    "\n",
    "    # Volume-based features\n",
    "    df['Volume_pct_change'] = df['Volume'].pct_change()\n",
    "    df['Volume_MA5'] = df['Volume'].rolling(5).mean()\n",
    "\n",
    "    # Lag features\n",
    "    df['Return_lag1'] = df['Return'].shift(1)\n",
    "    df['Return_lag2'] = df['Return'].shift(2)\n",
    "    df['Volatility_lag1'] = df['Volatility'].shift(1)\n",
    "\n",
    "    # Target: 10 days-ahead volatility\n",
    "    df['Volatility_future'] = df['Volatility'].shift(-10)\n",
    "\n",
    "    # Replace inf with NaN\n",
    "    df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "    # Fill volume-related NaNs (common in early data)\n",
    "    df['Volume_pct_change'] = df['Volume_pct_change'].fillna(0)\n",
    "    df['Volume_MA5'] = df['Volume_MA5'].ffill()\n",
    "\n",
    "    # Drop only rows where the key features are missing\n",
    "    df = df[df['Volatility'].notna() & df['Volatility_future'].notna()]\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # Features to scale\n",
    "    features = [\n",
    "        'Return', 'High_Low_pct', 'Close_Open_pct', 'MA5', 'MA10',\n",
    "        'Volume_pct_change', 'Volume_MA5', 'Return_lag1', 'Return_lag2', 'Volatility_lag1'\n",
    "    ]\n",
    "\n",
    "    # Scaling\n",
    "    scaler = StandardScaler()\n",
    "    df[features] = scaler.fit_transform(df[features].values)\n",
    "\n",
    "    target_col = 'Volatility_future'\n",
    "    return df, features, target_col\n",
    "\n",
    "\n",
    "# Load raw data\n",
    "df = load_csv(\"SPX.csv\")\n",
    "\n",
    "# Preprocess\n",
    "df_prepared, feature_cols, target_col = add_features(df)\n",
    "\n",
    "X = df_prepared[feature_cols]\n",
    "y = df_prepared[target_col]\n",
    "\n",
    "# Drop any row where X or y has NaN\n",
    "mask = (~X.isna().any(axis=1)) & (~y.isna())\n",
    "X = X[mask]\n",
    "y = y[mask]\n",
    "\n",
    "print(X.head(), y.head())\n",
    "\n",
    "\n",
    "# Split data into train (80%) and test (20%) by time\n",
    "split_idx = int(len(df_prepared) * 0.8)\n",
    "\n",
    "X_train, X_test = X.iloc[:split_idx], X.iloc[split_idx:]\n",
    "y_train, y_test = y.iloc[:split_idx], y.iloc[split_idx:]\n",
    "\n",
    "print(f\"Train size: {len(X_train)}, Test size: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7acd93f2-7bef-4f97-a4f3-8c6095962daf",
   "metadata": {},
   "source": [
    "## Hyperparameter Optimization with Keras Tuner\n",
    "\n",
    "### Purpose\n",
    "Automatically search for the optimal LSTM architecture and training configuration using the Hyperband algorithm—a smart resource allocation method that efficiently explores the hyperparameter space.\n",
    "\n",
    "### Model Building Function (`build_model`)\n",
    "\n",
    "The function defines a flexible LSTM architecture where Keras Tuner can optimize:\n",
    "\n",
    "**Architecture Parameters:**\n",
    "- **LSTM Units**: 32-256 neurons in first layer (step: 32)\n",
    "- **Second LSTM Layer**: Optional stacked LSTM (16-128 units)\n",
    "- **Dense Layer**: Optional fully-connected layer before output (16-64 units)\n",
    "- **Batch Normalization**: Optional after LSTM layers for training stability\n",
    "\n",
    "**Regularization (Prevent Overfitting):**\n",
    "- **Dropout**: 0.0-0.5 (randomly drops neurons during training)\n",
    "- **Recurrent Dropout**: 0.0-0.3 (dropout on LSTM recurrent connections)\n",
    "- **Weight Regularization**: None, L1, L2, or L1+L2 (elastic net)\n",
    "  - L1: Promotes sparsity (some weights → 0)\n",
    "  - L2: Prevents large weights (most common)\n",
    "  - Strength: 10⁻⁵ to 10⁻² (log scale)\n",
    "\n",
    "**Training Parameters:**\n",
    "- **Optimizer**: Adam, RMSprop, or SGD (with momentum if SGD)\n",
    "- **Learning Rate**: 10⁻⁴ to 10⁻² (log scale)\n",
    "- **Loss Function**: \n",
    "  - MSE (penalizes large errors heavily)\n",
    "  - MAE (robust to outliers)\n",
    "  - Huber (combines MSE + MAE)\n",
    "  - LogCosh (smooth MAE approximation)\n",
    "\n",
    "### Hyperband Algorithm\n",
    "\n",
    "**How it works:**\n",
    "1. Trains many configurations with few epochs\n",
    "2. Eliminates worst performers (keeps top 1/3)\n",
    "3. Gives survivors more training epochs\n",
    "4. Repeats until finding the best model\n",
    "\n",
    "**Configuration:**\n",
    "- `max_epochs=50`: Maximum training epochs for top candidates\n",
    "- `factor=3`: Keeps top 1/3 each round (aggressive pruning)\n",
    "- `validation_split=0.2`: Uses 20% of training data for validation\n",
    "\n",
    "**Callbacks:**\n",
    "- **EarlyStopping**: Stops if validation loss doesn't improve for 7 epochs\n",
    "- **ReduceLROnPlateau**: Cuts learning rate by half if stuck for 3 epochs\n",
    "\n",
    "### Why This Matters\n",
    "\n",
    "Instead of manually testing hundreds of configurations (which could take hours), Hyperband intelligently allocates compute time—spending more resources on promising models and quickly discarding poor ones. This typically finds near-optimal hyperparameters in hours rather than days.\n",
    "\n",
    "### Output\n",
    "\n",
    "The search returns the best hyperparameter configuration, which is then used to train the final model on the full training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dc559fe3-54e5-4315-b23e-9cb9316a8656",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 90 Complete [00h 02m 25s]\n",
      "val_loss: 0.010515527799725533\n",
      "\n",
      "Best val_loss So Far: 1.238825552718481e-05\n",
      "Total elapsed time: 00h 24m 17s\n",
      "\n",
      "======================================================================\n",
      "BEST HYPERPARAMETERS FOUND:\n",
      "======================================================================\n",
      "LSTM Units: 64\n",
      "Dropout Rate: 0.20\n",
      "Recurrent Dropout: 0.00\n",
      "Regularization Type: none\n",
      "Regularization Strength: 0.000022\n",
      "Optimizer: adam\n",
      "Learning Rate: 0.001075\n",
      "Loss Function: huber\n",
      "  Huber Delta: 2.00\n",
      "Batch Normalization (Layer 1): False\n",
      "Add Second LSTM Layer: True\n",
      "  Second Layer Units: 96\n",
      "  Batch Normalization (Layer 2): False\n",
      "Add Dense Layer: False\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "### Define the model building function\n",
    "def build_model(hp):\n",
    "    model = Sequential()\n",
    "    \n",
    "    # Tune number of LSTM units\n",
    "    units = hp.Int('units', min_value=32, max_value=256, step=32)\n",
    "    \n",
    "    # Tune dropout rates\n",
    "    dropout = hp.Float('dropout', min_value=0.0, max_value=0.5, step=0.1)\n",
    "    recurrent_dropout = hp.Float('recurrent_dropout', min_value=0.0, max_value=0.3, step=0.1)\n",
    "    \n",
    "    # Tune regularization type and strength\n",
    "    reg_type = hp.Choice('regularization_type', ['none', 'l1', 'l2', 'l1_l2'])\n",
    "    reg_strength = hp.Float('reg_strength', min_value=1e-5, max_value=1e-2, sampling='log')\n",
    "    \n",
    "    # Create regularizer based on choice\n",
    "    if reg_type == 'l1':\n",
    "        kernel_reg = regularizers.l1(reg_strength)\n",
    "        recurrent_reg = regularizers.l1(reg_strength * 0.1)  # Lighter on recurrent\n",
    "    elif reg_type == 'l2':\n",
    "        kernel_reg = regularizers.l2(reg_strength)\n",
    "        recurrent_reg = regularizers.l2(reg_strength * 0.1)\n",
    "    elif reg_type == 'l1_l2':\n",
    "        kernel_reg = regularizers.l1_l2(l1=reg_strength, l2=reg_strength)\n",
    "        recurrent_reg = regularizers.l1_l2(l1=reg_strength * 0.1, l2=reg_strength * 0.1)\n",
    "    else:\n",
    "        kernel_reg = None\n",
    "        recurrent_reg = None\n",
    "    \n",
    "    # Decide if we need a second LSTM layer first\n",
    "    add_second_layer = hp.Boolean('add_second_layer')\n",
    "    \n",
    "    # First LSTM layer with regularization\n",
    "    # Must return sequences if adding another LSTM layer\n",
    "    model.add(LSTM(\n",
    "        units=units,\n",
    "        input_shape=(X_train.shape[1], 1),\n",
    "        dropout=dropout,\n",
    "        recurrent_dropout=recurrent_dropout,\n",
    "        kernel_regularizer=kernel_reg,\n",
    "        recurrent_regularizer=recurrent_reg,\n",
    "        return_sequences=add_second_layer  # True only if adding second layer\n",
    "    ))\n",
    "    \n",
    "    # Optionally add batch normalization after first LSTM\n",
    "    if hp.Boolean('use_batch_norm_1'):\n",
    "        model.add(BatchNormalization())\n",
    "    \n",
    "    # Optionally add a second LSTM layer\n",
    "    if add_second_layer:\n",
    "        units_2 = hp.Int('units_2', min_value=16, max_value=128, step=16)\n",
    "        model.add(LSTM(\n",
    "            units=units_2,\n",
    "            dropout=dropout,\n",
    "            recurrent_dropout=recurrent_dropout,\n",
    "            kernel_regularizer=kernel_reg,\n",
    "            recurrent_regularizer=recurrent_reg,\n",
    "            return_sequences=False  # Final LSTM layer\n",
    "        ))\n",
    "        \n",
    "        if hp.Boolean('use_batch_norm_2'):\n",
    "            model.add(BatchNormalization())\n",
    "    \n",
    "    # Optionally add dense layer before output\n",
    "    if hp.Boolean('add_dense_layer'):\n",
    "        dense_units = hp.Int('dense_units', min_value=16, max_value=64, step=16)\n",
    "        model.add(Dense(\n",
    "            dense_units, \n",
    "            activation='relu',\n",
    "            kernel_regularizer=kernel_reg\n",
    "        ))\n",
    "        model.add(Dropout(dropout))\n",
    "    \n",
    "    # Output layer\n",
    "    model.add(Dense(1))\n",
    "    \n",
    "    # Tune learning rate\n",
    "    learning_rate = hp.Float('learning_rate', min_value=1e-4, max_value=1e-2, sampling='log')\n",
    "    \n",
    "    # Tune optimizer\n",
    "    optimizer_choice = hp.Choice('optimizer', ['adam', 'rmsprop', 'sgd'])\n",
    "    \n",
    "    if optimizer_choice == 'adam':\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    elif optimizer_choice == 'rmsprop':\n",
    "        optimizer = tf.keras.optimizers.RMSprop(learning_rate=learning_rate)\n",
    "    else:  # sgd\n",
    "        momentum = hp.Float('momentum', 0.0, 0.9, step=0.1)\n",
    "        optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate, momentum=momentum)\n",
    "    \n",
    "    # Tune loss function\n",
    "    loss_choice = hp.Choice('loss_function', ['mse', 'mae', 'huber', 'logcosh'])\n",
    "    \n",
    "    if loss_choice == 'huber':\n",
    "        loss = tf.keras.losses.Huber(delta=hp.Float('huber_delta', 0.5, 2.0, step=0.5))\n",
    "    else:\n",
    "        loss = loss_choice\n",
    "    \n",
    "    # Compile model\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=loss,\n",
    "        metrics=['mae', 'mse']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Create the tuner (using Hyperband algorithm)\n",
    "tuner = kt.Hyperband(\n",
    "    build_model,\n",
    "    objective='val_loss',\n",
    "    max_epochs=50,\n",
    "    factor=3,\n",
    "    directory='lstm_tuning_advanced',\n",
    "    project_name='hyperparameter_search_regularized',\n",
    "    overwrite=True\n",
    ")\n",
    "\n",
    "# Print search space summary\n",
    "print(\"Search Space Summary:\")\n",
    "print(tuner.search_space_summary())\n",
    "\n",
    "# Prepare data\n",
    "X_train_reshaped = X_train.values.reshape(-1, X_train.shape[1], 1)\n",
    "X_test_reshaped = X_test.values.reshape(-1, X_test.shape[1], 1)\n",
    "\n",
    "# Enhanced callbacks\n",
    "early_stop = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=7,\n",
    "    restore_best_weights=True,\n",
    "    min_delta=1e-6\n",
    ")\n",
    "\n",
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.5,\n",
    "    patience=3,\n",
    "    min_lr=1e-7,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "# Run the hyperparameter search\n",
    "print(\"\\nStarting hyperparameter search...\")\n",
    "tuner.search(\n",
    "    X_train_reshaped, \n",
    "    y_train.values,\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[early_stop, reduce_lr],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"BEST HYPERPARAMETERS FOUND:\")\n",
    "print(\"=\"*70)\n",
    "print(f\"LSTM Units: {best_hps.get('units')}\")\n",
    "print(f\"Dropout Rate: {best_hps.get('dropout'):.2f}\")\n",
    "print(f\"Recurrent Dropout: {best_hps.get('recurrent_dropout'):.2f}\")\n",
    "print(f\"Regularization Type: {best_hps.get('regularization_type')}\")\n",
    "print(f\"Regularization Strength: {best_hps.get('reg_strength'):.6f}\")\n",
    "print(f\"Optimizer: {best_hps.get('optimizer')}\")\n",
    "if best_hps.get('optimizer') == 'sgd':\n",
    "    print(f\"  Momentum: {best_hps.get('momentum'):.2f}\")\n",
    "print(f\"Learning Rate: {best_hps.get('learning_rate'):.6f}\")\n",
    "print(f\"Loss Function: {best_hps.get('loss_function')}\")\n",
    "if best_hps.get('loss_function') == 'huber':\n",
    "    print(f\"  Huber Delta: {best_hps.get('huber_delta'):.2f}\")\n",
    "print(f\"Batch Normalization (Layer 1): {best_hps.get('use_batch_norm_1')}\")\n",
    "print(f\"Add Second LSTM Layer: {best_hps.get('add_second_layer')}\")\n",
    "if best_hps.get('add_second_layer'):\n",
    "    print(f\"  Second Layer Units: {best_hps.get('units_2')}\")\n",
    "    print(f\"  Batch Normalization (Layer 2): {best_hps.get('use_batch_norm_2')}\")\n",
    "print(f\"Add Dense Layer: {best_hps.get('add_dense_layer')}\")\n",
    "if best_hps.get('add_dense_layer'):\n",
    "    print(f\"  Dense Layer Units: {best_hps.get('dense_units')}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e00094b-37b8-4e9a-82e4-1975b8a65dc0",
   "metadata": {},
   "source": [
    "## Training the Optimized Model\n",
    "\n",
    "### Process\n",
    "\n",
    "**1. Build Best Model**\n",
    "- Constructs the LSTM architecture using the optimal hyperparameters found by Keras Tuner\n",
    "- Alternatively, can load a previously saved model (commented line)\n",
    "\n",
    "**2. Train on Full Training Set**\n",
    "- Uses full 80% training data (no longer splitting for hyperparameter validation)\n",
    "- Validates against the held-out 20% test set to monitor generalization\n",
    "- Early stopping prevents overfitting (stops if test performance degrades)\n",
    "- Maximum 50 epochs, but typically stops earlier\n",
    "\n",
    "**3. Evaluation Metrics**\n",
    "- **Mean Squared Error (MSE)**: Average squared difference between predicted and actual volatility (lower is better)\n",
    "- **R² Score**: Proportion of variance explained by the model (0-1 scale, higher is better)\n",
    "  - R² = 0: Model performs no better than predicting the mean\n",
    "  - R² = 1: Perfect predictions\n",
    "  - R² = 0.57 means the model explains 57% of volatility variation\n",
    "\n",
    "### Baseline Comparison\n",
    "\n",
    "To demonstrate the value of hyperparameter optimization, we train a simple baseline LSTM:\n",
    "- **Original Model**: Single 50-unit LSTM layer, default Adam optimizer, MSE loss\n",
    "- **Tuned Model**: Optimized architecture with regularization, custom loss function, etc.\n",
    "\n",
    "The comparison shows percentage improvement in both MSE (prediction accuracy) and R² (variance explained).\n",
    "\n",
    "### Model Persistence\n",
    "\n",
    "The best model is saved in Keras native format (`.keras`) for future use:\n",
    "- Faster load times than legacy HDF5 format\n",
    "- Better compatibility with newer TensorFlow versions\n",
    "- Includes full architecture, weights, and optimizer state\n",
    "\n",
    "### Top Candidates\n",
    "\n",
    "Displays the top 3 hyperparameter configurations found during the search, useful for:\n",
    "- Understanding what patterns work well (e.g., does adding a second layer consistently help?)\n",
    "- Ensemble modeling (combining predictions from multiple good models)\n",
    "- Future experimentation starting points\n",
    "\n",
    "### Expected Results\n",
    "\n",
    "Typical improvements from hyperparameter tuning:\n",
    "- **20-40% reduction in MSE** (more accurate predictions)\n",
    "- **30-60% improvement in R²** (better variance capture)\n",
    "- More stable training (less prone to overfitting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "90d1fd82-2045-45d5-b95c-b4831acc2b3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training the best model...\n",
      "Epoch 1/50\n",
      "583/583 [==============================] - 3s 4ms/step - loss: 2.4361e-05 - mae: 0.0045 - mse: 4.8722e-05 - val_loss: 4.0011e-05 - val_mae: 0.0062 - val_mse: 8.0022e-05\n",
      "Epoch 2/50\n",
      "583/583 [==============================] - 2s 3ms/step - loss: 1.7745e-05 - mae: 0.0037 - mse: 3.5489e-05 - val_loss: 1.8304e-05 - val_mae: 0.0039 - val_mse: 3.6608e-05\n",
      "Epoch 3/50\n",
      "583/583 [==============================] - 2s 3ms/step - loss: 1.7123e-05 - mae: 0.0036 - mse: 3.4245e-05 - val_loss: 1.7092e-05 - val_mae: 0.0035 - val_mse: 3.4183e-05\n",
      "Epoch 4/50\n",
      "583/583 [==============================] - 2s 3ms/step - loss: 1.6462e-05 - mae: 0.0035 - mse: 3.2924e-05 - val_loss: 1.4342e-05 - val_mae: 0.0039 - val_mse: 2.8683e-05\n",
      "Epoch 5/50\n",
      "583/583 [==============================] - 2s 3ms/step - loss: 1.6016e-05 - mae: 0.0035 - mse: 3.2031e-05 - val_loss: 1.4678e-05 - val_mae: 0.0038 - val_mse: 2.9356e-05\n",
      "Epoch 6/50\n",
      "583/583 [==============================] - 2s 3ms/step - loss: 1.6020e-05 - mae: 0.0035 - mse: 3.2040e-05 - val_loss: 1.5194e-05 - val_mae: 0.0036 - val_mse: 3.0389e-05\n",
      "Epoch 7/50\n",
      "583/583 [==============================] - 2s 3ms/step - loss: 1.6209e-05 - mae: 0.0035 - mse: 3.2419e-05 - val_loss: 1.2588e-05 - val_mae: 0.0032 - val_mse: 2.5176e-05\n",
      "Epoch 8/50\n",
      "583/583 [==============================] - 2s 3ms/step - loss: 1.5864e-05 - mae: 0.0035 - mse: 3.1727e-05 - val_loss: 1.6227e-05 - val_mae: 0.0043 - val_mse: 3.2455e-05\n",
      "Epoch 9/50\n",
      "583/583 [==============================] - 2s 3ms/step - loss: 1.5891e-05 - mae: 0.0035 - mse: 3.1783e-05 - val_loss: 1.6484e-05 - val_mae: 0.0036 - val_mse: 3.2967e-05\n",
      "Epoch 10/50\n",
      "583/583 [==============================] - 2s 3ms/step - loss: 1.5613e-05 - mae: 0.0034 - mse: 3.1225e-05 - val_loss: 1.6158e-05 - val_mae: 0.0039 - val_mse: 3.2316e-05\n",
      "Epoch 11/50\n",
      "583/583 [==============================] - 2s 3ms/step - loss: 1.5677e-05 - mae: 0.0035 - mse: 3.1354e-05 - val_loss: 1.6591e-05 - val_mae: 0.0036 - val_mse: 3.3183e-05\n",
      "Epoch 12/50\n",
      "583/583 [==============================] - 2s 3ms/step - loss: 1.5633e-05 - mae: 0.0034 - mse: 3.1266e-05 - val_loss: 1.4983e-05 - val_mae: 0.0040 - val_mse: 2.9965e-05\n",
      "Epoch 13/50\n",
      "583/583 [==============================] - 2s 3ms/step - loss: 1.5593e-05 - mae: 0.0034 - mse: 3.1185e-05 - val_loss: 1.6346e-05 - val_mae: 0.0036 - val_mse: 3.2691e-05\n",
      "Epoch 14/50\n",
      "583/583 [==============================] - 2s 3ms/step - loss: 1.5128e-05 - mae: 0.0034 - mse: 3.0257e-05 - val_loss: 1.5567e-05 - val_mae: 0.0037 - val_mse: 3.1134e-05\n",
      "146/146 [==============================] - 0s 1ms/step\n",
      "\n",
      "============================================================\n",
      "FINAL MODEL PERFORMANCE:\n",
      "============================================================\n",
      "Mean Squared Error: 0.000025\n",
      "R² Score: 0.581421\n",
      "Accuracy (R²): 58.14%\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "COMPARISON WITH ORIGINAL MODEL:\n",
      "============================================================\n",
      "146/146 [==============================] - 0s 510us/step\n",
      "Original Model - MSE: 0.000036, R²: 39.81%\n",
      "Tuned Model    - MSE: 0.000025, R²: 58.14%\n",
      "Improvement    - MSE: 30.45%\n",
      "Improvement    - R²: 46.04%\n",
      "============================================================\n",
      "\n",
      "Best model saved as 'best_lstm_model.keras'\n",
      "\n",
      "============================================================\n",
      "TOP 3 MODELS:\n",
      "============================================================\n",
      "\n",
      "Model 1:\n",
      "  Units: 64, Dropout: 0.20\n",
      "  Learning Rate: 0.001075\n",
      "  Second Layer: True\n",
      "\n",
      "Model 2:\n",
      "  Units: 64, Dropout: 0.20\n",
      "  Learning Rate: 0.001075\n",
      "  Second Layer: True\n",
      "\n",
      "Model 3:\n",
      "  Units: 224, Dropout: 0.20\n",
      "  Learning Rate: 0.001106\n",
      "  Second Layer: False\n"
     ]
    }
   ],
   "source": [
    "### Build and train the best model\n",
    "print(\"\\nTraining the best model...\")\n",
    "best_model = tuner.hypermodel.build(best_hps)\n",
    "#best_model = load_model('best_lstm_model.keras')\n",
    "\n",
    "history = best_model.fit(\n",
    "    X_train_reshaped,\n",
    "    y_train.values,\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    validation_data=(X_test_reshaped, y_test.values),\n",
    "    callbacks=[early_stop],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = best_model.predict(X_test_reshaped)\n",
    "\n",
    "# Evaluate performance\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# Print results\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FINAL MODEL PERFORMANCE:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Mean Squared Error: {mse:.6f}\")\n",
    "print(f\"R² Score: {r2:.6f}\")\n",
    "print(f\"Accuracy (R²): {r2 * 100:.2f}%\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Compare with original model\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"COMPARISON WITH ORIGINAL MODEL:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Train original, simple LSTM model for comparison\n",
    "original_model = Sequential([\n",
    "    LSTM(50, input_shape=(X_train.shape[1], 1)),\n",
    "    Dense(1)\n",
    "])\n",
    "original_model.compile(optimizer='adam', loss='mse')\n",
    "original_model.fit(\n",
    "    X_train_reshaped, \n",
    "    y_train.values, \n",
    "    epochs=50, \n",
    "    batch_size=32, \n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "y_pred_original = original_model.predict(X_test_reshaped)\n",
    "mse_original = mean_squared_error(y_test, y_pred_original)\n",
    "r2_original = r2_score(y_test, y_pred_original)\n",
    "\n",
    "print(f\"Original Model - MSE: {mse_original:.6f}, R²: {r2_original * 100:.2f}%\")\n",
    "print(f\"Tuned Model    - MSE: {mse:.6f}, R²: {r2 * 100:.2f}%\")\n",
    "print(f\"Improvement    - MSE: {((mse_original - mse) / mse_original * 100):.2f}%\")\n",
    "print(f\"Improvement    - R²: {((r2 - r2_original) / abs(r2_original) * 100):.2f}%\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Save the best model\n",
    "best_model.save('best_lstm_model.keras')\n",
    "print(\"\\nBest model saved as 'best_lstm_model.keras'\")\n",
    "\n",
    "# Show top 3 models\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TOP 3 MODELS:\")\n",
    "print(\"=\"*60)\n",
    "for i, hps in enumerate(tuner.get_best_hyperparameters(num_trials=3), 1):\n",
    "    print(f\"\\nModel {i}:\")\n",
    "    print(f\"  Units: {hps.get('units')}, Dropout: {hps.get('dropout'):.2f}\")\n",
    "    print(f\"  Learning Rate: {hps.get('learning_rate'):.6f}\")\n",
    "    print(f\"  Second Layer: {hps.get('add_second_layer')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aa07ac2-c261-48fb-ad8c-49378789138b",
   "metadata": {},
   "source": [
    "## Model Performance Visualization and Diagnostics\n",
    "\n",
    "### Purpose\n",
    "Generate 12 detailed diagnostic plots to thoroughly evaluate the LSTM model's predictive performance from multiple angles. All plots are saved as high-resolution images (300 DPI) in the `figures/` directory.\n",
    "\n",
    "### Diagnostic Plots Generated\n",
    "\n",
    "**1. Actual vs Predicted Timeline**\n",
    "- Time series overlay showing how well predictions track actual volatility\n",
    "- Shaded area visualizes prediction errors\n",
    "- Includes key metrics (R², RMSE, MAE) in text box\n",
    "\n",
    "**2. Scatter Plot: Predicted vs Actual**\n",
    "- Each point represents one prediction\n",
    "- Red dashed line = perfect predictions\n",
    "- Green line = actual fitted relationship\n",
    "- Correlation coefficient shows linear relationship strength\n",
    "\n",
    "**3. Residuals Over Time**\n",
    "- Shows prediction errors (actual - predicted) chronologically\n",
    "- Helps identify systematic biases or time-dependent patterns\n",
    "- ±2σ bands indicate 95% expected error range\n",
    "- Clustering outside bands suggests model struggles in certain periods\n",
    "\n",
    "**4. Residuals Distribution**\n",
    "- Histogram of prediction errors with normal curve overlay\n",
    "- **Skewness**: Measures asymmetry (0 = symmetric)\n",
    "  - Positive: More large overestimations\n",
    "  - Negative: More large underestimations\n",
    "- **Kurtosis**: Measures tail heaviness (0 = normal)\n",
    "  - Positive: More extreme errors than expected\n",
    "  - Negative: Fewer extreme errors\n",
    "\n",
    "**5. Q-Q Plot (Quantile-Quantile)**\n",
    "- Tests if residuals follow normal distribution\n",
    "- Points on diagonal = normally distributed errors (ideal for regression)\n",
    "- Deviations indicate non-normal error patterns\n",
    "\n",
    "**6. Absolute Error Timeline**\n",
    "- Magnitude of errors over time (ignoring direction)\n",
    "- Rolling 50-period average smooths short-term noise\n",
    "- Identifies periods where model struggles most\n",
    "\n",
    "**7. Percentage Error Distribution**\n",
    "- Errors expressed as % of actual volatility\n",
    "- MAPE (Mean Absolute Percentage Error) = average % error\n",
    "- More interpretable than absolute errors for stakeholders\n",
    "\n",
    "**8. Error vs Volatility Level**\n",
    "- Scatter showing if errors increase with volatility magnitude\n",
    "- Positive trend = model struggles more in high volatility periods\n",
    "- Flat trend = consistent accuracy across volatility regimes\n",
    "\n",
    "**9. Accuracy by Volatility Quartile**\n",
    "- Splits test data into 4 groups (low, med-low, med-high, high volatility)\n",
    "- Shows MAE and R² for each quartile\n",
    "- Identifies which market conditions model handles best/worst\n",
    "\n",
    "**10. Cumulative Absolute Error**\n",
    "- Running sum of all errors over time\n",
    "- Steady linear growth = consistent error rate\n",
    "- Accelerating growth = model degrading over time\n",
    "- Useful for assessing long-term reliability\n",
    "\n",
    "**11. Error Autocorrelation**\n",
    "- Tests if errors are independent across time\n",
    "- Significant autocorrelation = errors are predictable (model missing patterns)\n",
    "- Random pattern = model captured all learnable structure\n",
    "\n",
    "**12. Predictions with Confidence Intervals**\n",
    "- Shows predictions with ±2σ uncertainty bands\n",
    "- Wider bands = less confidence\n",
    "- Helps quantify prediction reliability for decision-making\n",
    "\n",
    "### Statistical Summary\n",
    "\n",
    "Comprehensive metrics printed to console:\n",
    "- **Regression Metrics**: R², MSE, RMSE, MAE, MAPE\n",
    "- **Residual Statistics**: Mean, median, std dev, min/max, skewness, kurtosis\n",
    "- **Error Distribution**: % of predictions within 1 MAE, 2 MAE, 1σ, 2σ\n",
    "- **Quartile Analysis**: Performance breakdown by volatility level\n",
    "\n",
    "### Interpretation Guidelines\n",
    "\n",
    "**Good Model Indicators:**\n",
    "- High R² (>0.5 for financial data is strong)\n",
    "- Residuals centered near zero (mean ≈ 0)\n",
    "- Normally distributed errors (Q-Q plot linear)\n",
    "- No autocorrelation in residuals\n",
    "- Consistent accuracy across volatility quartiles\n",
    "\n",
    "**Warning Signs:**\n",
    "- Systematic bias (residual mean far from 0)\n",
    "- Heavy tails (kurtosis >> 0) = unexpected large errors\n",
    "- Strong autocorrelation = missing temporal patterns\n",
    "- Degrading cumulative error = model unstable over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ad0e50b8-2a03-4ae7-904e-0b5ab250c0c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "GENERATING INDIVIDUAL PLOTS...\n",
      "======================================================================\n",
      "\n",
      "1. Creating Actual vs Predicted timeline...\n",
      "2. Creating scatter plot...\n",
      "3. Creating residuals timeline...\n",
      "4. Creating residuals distribution...\n",
      "5. Creating Q-Q plot...\n",
      "6. Creating absolute error timeline...\n",
      "7. Creating percentage error distribution...\n",
      "8. Creating error vs volatility level...\n",
      "9. Creating accuracy by quartile...\n",
      "10. Creating cumulative error...\n",
      "11. Creating error autocorrelation...\n",
      "12. Creating prediction with confidence intervals...\n",
      "\n",
      "======================================================================\n",
      "DETAILED MODEL PERFORMANCE STATISTICS\n",
      "======================================================================\n",
      "\n",
      "Regression Metrics:\n",
      "  R² Score:                    0.581421\n",
      "  Mean Squared Error (MSE):    0.000025\n",
      "  Root Mean Squared Error:     0.005018\n",
      "  Mean Absolute Error (MAE):   0.003211\n",
      "  Mean Absolute % Error:       40.45%\n",
      "\n",
      "Residual Statistics:\n",
      "  Mean:                        0.000144\n",
      "  Median:                      -0.000628\n",
      "  Std Deviation:               0.005016\n",
      "  Min Error:                   -0.012425\n",
      "  Max Error:                   0.050183\n",
      "  Skewness:                    3.0175\n",
      "  Kurtosis:                    17.8339\n",
      "\n",
      "Error Distribution:\n",
      "  % predictions within 1 MAE:  63.93%\n",
      "  % predictions within 2 MAE:  90.90%\n",
      "  % predictions within 1σ:     83.58%\n",
      "  % predictions within 2σ:     96.42%\n",
      "\n",
      "Prediction Quality by Quartile:\n",
      "  Q3: MAE = 0.002188, R² = -4.3729\n",
      "  Q4 (High): MAE = 0.005813, R² = 0.2559\n",
      "  Q2: MAE = 0.001614, R² = -10.4284\n",
      "  Q1 (Low): MAE = 0.003230, R² = -16.6070\n",
      "\n",
      "======================================================================\n",
      "ALL PLOTS SAVED SUCCESSFULLY!\n",
      "======================================================================\n",
      "\n",
      "Generated files in 'figures/' directory:\n",
      "  01_*.png\n",
      "  02_*.png\n",
      "  03_*.png\n",
      "  04_*.png\n",
      "  05_*.png\n",
      "  06_*.png\n",
      "  07_*.png\n",
      "  08_*.png\n",
      "  09_*.png\n",
      "  10_*.png\n",
      "  11_*.png\n",
      "  12_*.png\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Set style for better-looking plots\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Dates from dataframe:\n",
    "test_dates = df_prepared['Date'].iloc[split_idx:-1].values\n",
    "\n",
    "# Calculate metrics\n",
    "y_pred = y_pred.flatten()\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# Calculate residuals\n",
    "residuals = y_test - y_pred.flatten()\n",
    "residuals_pct = (residuals / y_test) * 100\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"GENERATING INDIVIDUAL PLOTS...\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# ============================================================================\n",
    "# Plot 1: Actual vs Predicted Over Time (Main Plot)\n",
    "# ============================================================================\n",
    "print(\"\\n1. Creating Actual vs Predicted timeline...\")\n",
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "ax.plot(test_dates, y_test, label='Actual Volatility', linewidth=2, alpha=0.8, color='#2E86AB')\n",
    "ax.plot(test_dates, y_pred, label='Predicted Volatility', linewidth=2, alpha=0.8, \n",
    "        color='#A23B72', linestyle='--')\n",
    "ax.fill_between(test_dates, y_test, y_pred, alpha=0.2, color='gray', label='Prediction Error')\n",
    "ax.set_title('Actual vs Predicted 10-Day Ahead Volatility', fontsize=16, fontweight='bold', pad=20)\n",
    "ax.set_xlabel('Date', fontsize=12)\n",
    "ax.set_ylabel('Volatility', fontsize=12)\n",
    "ax.legend(loc='upper left', fontsize=11, framealpha=0.9)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Add text box with metrics\n",
    "textstr = f'R² = {r2:.4f}\\nRMSE = {rmse:.6f}\\nMAE = {mae:.6f}'\n",
    "props = dict(boxstyle='round', facecolor='wheat', alpha=0.8)\n",
    "ax.text(0.98, 0.97, textstr, transform=ax.transAxes, fontsize=11,\n",
    "        verticalalignment='top', horizontalalignment='right', bbox=props)\n",
    "\n",
    "ax.tick_params(axis='x', rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.savefig('figures/01_actual_vs_predicted_timeline.png', dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "# ============================================================================\n",
    "# Plot 2: Scatter Plot - Actual vs Predicted\n",
    "# ============================================================================\n",
    "print(\"2. Creating scatter plot...\")\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "ax.scatter(y_test, y_pred, alpha=0.5, s=30, edgecolors='k', linewidth=0.5)\n",
    "\n",
    "# Add perfect prediction line\n",
    "min_val = min(y_test.min(), y_pred.min())\n",
    "max_val = max(y_test.max(), y_pred.max())\n",
    "ax.plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2.5, label='Perfect Prediction')\n",
    "\n",
    "# Add regression line\n",
    "z = np.polyfit(y_test, y_pred.flatten(), 1)\n",
    "p = np.poly1d(z)\n",
    "ax.plot(y_test, p(y_test), \"g-\", alpha=0.8, linewidth=2.5, label=f'Fit: y={z[0]:.3f}x+{z[1]:.6f}')\n",
    "\n",
    "ax.set_xlabel('Actual Volatility', fontsize=12)\n",
    "ax.set_ylabel('Predicted Volatility', fontsize=12)\n",
    "ax.set_title('Predicted vs Actual Scatter Plot', fontsize=16, fontweight='bold')\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Add correlation coefficient\n",
    "corr = np.corrcoef(y_test, y_pred.flatten())[0, 1]\n",
    "ax.text(0.05, 0.95, f'Correlation: {corr:.4f}', transform=ax.transAxes,\n",
    "        fontsize=11, verticalalignment='top', bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.8))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('figures/02_scatter_actual_vs_predicted.png', dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "# ============================================================================\n",
    "# Plot 3: Residuals Over Time\n",
    "# ============================================================================\n",
    "print(\"3. Creating residuals timeline...\")\n",
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "ax.plot(test_dates, residuals, linewidth=1, alpha=0.7, color='darkred')\n",
    "ax.axhline(y=0, color='black', linestyle='-', linewidth=1.5)\n",
    "ax.axhline(y=residuals.mean(), color='blue', linestyle='--', linewidth=1.5, \n",
    "           label=f'Mean: {residuals.mean():.6f}')\n",
    "ax.fill_between(test_dates, 0, residuals, alpha=0.3, color='darkred')\n",
    "\n",
    "# Add confidence bands (±2 std)\n",
    "std_residuals = residuals.std()\n",
    "ax.axhline(y=2*std_residuals, color='orange', linestyle=':', linewidth=1.5, alpha=0.7, label='±2σ')\n",
    "ax.axhline(y=-2*std_residuals, color='orange', linestyle=':', linewidth=1.5, alpha=0.7)\n",
    "\n",
    "ax.set_xlabel('Date', fontsize=12)\n",
    "ax.set_ylabel('Residuals', fontsize=12)\n",
    "ax.set_title('Prediction Errors Over Time', fontsize=16, fontweight='bold')\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('figures/03_residuals_timeline.png', dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "# ============================================================================\n",
    "# Plot 4: Residuals Distribution\n",
    "# ============================================================================\n",
    "print(\"4. Creating residuals distribution...\")\n",
    "fig, ax = plt.subplots(figsize=(10, 7))\n",
    "ax.hist(residuals, bins=50, density=True, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "\n",
    "# Overlay normal distribution\n",
    "mu, sigma = residuals.mean(), residuals.std()\n",
    "x = np.linspace(residuals.min(), residuals.max(), 100)\n",
    "ax.plot(x, stats.norm.pdf(x, mu, sigma), 'r-', linewidth=2.5, label='Normal Distribution')\n",
    "\n",
    "ax.set_xlabel('Residuals', fontsize=12)\n",
    "ax.set_ylabel('Density', fontsize=12)\n",
    "ax.set_title('Residuals Distribution', fontsize=16, fontweight='bold')\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add skewness and kurtosis\n",
    "skew = stats.skew(residuals)\n",
    "kurt = stats.kurtosis(residuals)\n",
    "textstr = f'Skewness: {skew:.3f}\\nKurtosis: {kurt:.3f}\\nMean: {mu:.6f}\\nStd Dev: {sigma:.6f}'\n",
    "ax.text(0.95, 0.95, textstr, transform=ax.transAxes, fontsize=10,\n",
    "        verticalalignment='top', horizontalalignment='right',\n",
    "        bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('figures/04_residuals_distribution.png', dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "# ============================================================================\n",
    "# Plot 5: Q-Q Plot\n",
    "# ============================================================================\n",
    "print(\"5. Creating Q-Q plot...\")\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "stats.probplot(residuals, dist=\"norm\", plot=ax)\n",
    "ax.set_title('Q-Q Plot (Normality Check)', fontsize=16, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.get_lines()[0].set_markerfacecolor('skyblue')\n",
    "ax.get_lines()[0].set_markeredgecolor('black')\n",
    "ax.get_lines()[0].set_markersize(6)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('figures/05_qq_plot.png', dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "# ============================================================================\n",
    "# Plot 6: Absolute Error Over Time\n",
    "# ============================================================================\n",
    "print(\"6. Creating absolute error timeline...\")\n",
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "abs_errors = np.abs(residuals)\n",
    "ax.plot(test_dates, abs_errors, linewidth=1, alpha=0.7, color='purple', label='Absolute Error')\n",
    "ax.axhline(y=mae, color='red', linestyle='--', linewidth=2, label=f'MAE: {mae:.6f}')\n",
    "\n",
    "# Add rolling mean of absolute errors\n",
    "window = 50\n",
    "rolling_mae = pd.Series(abs_errors).rolling(window=window, min_periods=1).mean()\n",
    "ax.plot(test_dates, rolling_mae, linewidth=2.5, alpha=0.8, color='orange',\n",
    "        label=f'Rolling MAE (window={window})')\n",
    "\n",
    "ax.set_xlabel('Date', fontsize=12)\n",
    "ax.set_ylabel('Absolute Error', fontsize=12)\n",
    "ax.set_title('Absolute Prediction Error Over Time', fontsize=16, fontweight='bold')\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('figures/06_absolute_error_timeline.png', dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "# ============================================================================\n",
    "# Plot 7: Percentage Error Distribution\n",
    "# ============================================================================\n",
    "print(\"7. Creating percentage error distribution...\")\n",
    "fig, ax = plt.subplots(figsize=(10, 7))\n",
    "# Remove outliers for better visualization\n",
    "pct_errors_clean = residuals_pct[np.abs(residuals_pct) < np.percentile(np.abs(residuals_pct), 95)]\n",
    "\n",
    "ax.hist(pct_errors_clean, bins=50, alpha=0.7, color='lightcoral', edgecolor='black')\n",
    "ax.axvline(x=0, color='black', linestyle='-', linewidth=2.5)\n",
    "ax.axvline(x=np.median(residuals_pct), color='blue', linestyle='--', linewidth=2,\n",
    "           label=f'Median: {np.median(residuals_pct):.2f}%')\n",
    "\n",
    "ax.set_xlabel('Percentage Error (%)', fontsize=12)\n",
    "ax.set_ylabel('Frequency', fontsize=12)\n",
    "ax.set_title('Percentage Prediction Error Distribution', fontsize=16, fontweight='bold')\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add MAPE\n",
    "mape = np.mean(np.abs(residuals_pct))\n",
    "textstr = f'MAPE: {mape:.2f}%\\nMedian: {np.median(residuals_pct):.2f}%'\n",
    "ax.text(0.95, 0.95, textstr, transform=ax.transAxes, fontsize=11,\n",
    "        verticalalignment='top', horizontalalignment='right',\n",
    "        bbox=dict(boxstyle='round', facecolor='lightgreen', alpha=0.8))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('figures/07_percentage_error_distribution.png', dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "# ============================================================================\n",
    "# Plot 8: Error by Volatility Level\n",
    "# ============================================================================\n",
    "print(\"8. Creating error vs volatility level...\")\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "ax.scatter(y_test, np.abs(residuals), alpha=0.5, s=30, edgecolors='k', linewidth=0.5)\n",
    "ax.set_xlabel('Actual Volatility Level', fontsize=12)\n",
    "ax.set_ylabel('Absolute Error', fontsize=12)\n",
    "ax.set_title('Prediction Error vs Volatility Level', fontsize=16, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Add trend line\n",
    "z = np.polyfit(y_test, np.abs(residuals), 1)\n",
    "p = np.poly1d(z)\n",
    "sorted_y = np.sort(y_test)\n",
    "ax.plot(sorted_y, p(sorted_y), \"r--\", linewidth=2.5, alpha=0.8, \n",
    "        label=f'Trend: y={z[0]:.6f}x+{z[1]:.6f}')\n",
    "ax.legend(fontsize=11)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('figures/08_error_vs_volatility_level.png', dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "# ============================================================================\n",
    "# Plot 9: Prediction Accuracy by Quartile\n",
    "# ============================================================================\n",
    "print(\"9. Creating accuracy by quartile...\")\n",
    "quartiles = pd.qcut(y_test, q=4, labels=['Q1 (Low)', 'Q2', 'Q3', 'Q4 (High)'])\n",
    "quartile_mae = [np.mean(np.abs(residuals[quartiles == q])) for q in quartiles.unique()]\n",
    "quartile_r2 = [r2_score(y_test[quartiles == q], y_pred.flatten()[quartiles == q]) \n",
    "               for q in quartiles.unique()]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 7))\n",
    "x_pos = np.arange(len(quartiles.unique()))\n",
    "bars = ax.bar(x_pos, quartile_mae, alpha=0.7, color='steelblue', edgecolor='black', width=0.6)\n",
    "ax.set_xlabel('Volatility Quartile', fontsize=12)\n",
    "ax.set_ylabel('Mean Absolute Error', fontsize=12)\n",
    "ax.set_title('Prediction Error by Volatility Quartile', fontsize=16, fontweight='bold')\n",
    "ax.set_xticks(x_pos)\n",
    "ax.set_xticklabels(quartiles.unique())\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add values on bars\n",
    "for i, (v, r2_val) in enumerate(zip(quartile_mae, quartile_r2)):\n",
    "    ax.text(i, v + max(quartile_mae)*0.02, f'MAE: {v:.6f}\\nR²: {r2_val:.3f}', \n",
    "            ha='center', va='bottom', fontsize=9, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('figures/09_accuracy_by_quartile.png', dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "# ============================================================================\n",
    "# Plot 10: Cumulative Error\n",
    "# ============================================================================\n",
    "print(\"10. Creating cumulative error...\")\n",
    "cumulative_error = np.cumsum(np.abs(residuals))\n",
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "ax.plot(test_dates, cumulative_error, linewidth=2.5, color='darkgreen')\n",
    "ax.set_xlabel('Date', fontsize=12)\n",
    "ax.set_ylabel('Cumulative Absolute Error', fontsize=12)\n",
    "ax.set_title('Cumulative Prediction Error', fontsize=16, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Add slope line\n",
    "total_error = cumulative_error.iloc[-1] if hasattr(cumulative_error, 'iloc') else cumulative_error[-1]\n",
    "avg_rate = total_error / len(cumulative_error)\n",
    "ax.plot(test_dates, np.arange(len(test_dates)) * avg_rate, 'r--', linewidth=2, \n",
    "        label=f'Average rate: {avg_rate:.6f}/step')\n",
    "ax.legend(fontsize=11)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('figures/10_cumulative_error.png', dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "# ============================================================================\n",
    "# Plot 11: Error Autocorrelation\n",
    "# ============================================================================\n",
    "print(\"11. Creating error autocorrelation...\")\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "from pandas.plotting import autocorrelation_plot\n",
    "autocorrelation_plot(pd.Series(residuals), ax=ax)\n",
    "ax.set_title('Residuals Autocorrelation', fontsize=16, fontweight='bold')\n",
    "ax.set_xlabel('Lag', fontsize=12)\n",
    "ax.set_ylabel('Autocorrelation', fontsize=12)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('figures/11_error_autocorrelation.png', dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "# ============================================================================\n",
    "# Plot 12: Prediction Confidence Intervals\n",
    "# ============================================================================\n",
    "print(\"12. Creating prediction with confidence intervals...\")\n",
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "\n",
    "# Plot every Nth point to avoid overcrowding\n",
    "step = max(1, len(test_dates) // 200)\n",
    "dates_sample = test_dates[::step]\n",
    "y_test_sample = y_test[::step]\n",
    "y_pred_sample = y_pred[::step].flatten()\n",
    "residuals_sample = residuals[::step]\n",
    "\n",
    "# Calculate prediction intervals (±2 std)\n",
    "prediction_interval = 2 * std_residuals\n",
    "\n",
    "ax.plot(dates_sample, y_test_sample, 'o-', label='Actual', linewidth=2, markersize=4, alpha=0.8)\n",
    "ax.plot(dates_sample, y_pred_sample, 's-', label='Predicted', linewidth=2, markersize=4, alpha=0.8)\n",
    "ax.fill_between(dates_sample, \n",
    "                y_pred_sample - prediction_interval,\n",
    "                y_pred_sample + prediction_interval,\n",
    "                alpha=0.2, color='gray', label='95% Prediction Interval')\n",
    "\n",
    "ax.set_xlabel('Date', fontsize=12)\n",
    "ax.set_ylabel('Volatility', fontsize=12)\n",
    "ax.set_title('Predictions with 95% Confidence Intervals', fontsize=16, fontweight='bold')\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('figures/12_predictions_with_confidence_intervals.png', dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "# ============================================================================\n",
    "# Print detailed statistics\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"DETAILED MODEL PERFORMANCE STATISTICS\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nRegression Metrics:\")\n",
    "print(f\"  R² Score:                    {r2:.6f}\")\n",
    "print(f\"  Mean Squared Error (MSE):    {mse:.6f}\")\n",
    "print(f\"  Root Mean Squared Error:     {rmse:.6f}\")\n",
    "print(f\"  Mean Absolute Error (MAE):   {mae:.6f}\")\n",
    "print(f\"  Mean Absolute % Error:       {mape:.2f}%\")\n",
    "\n",
    "print(f\"\\nResidual Statistics:\")\n",
    "print(f\"  Mean:                        {residuals.mean():.6f}\")\n",
    "print(f\"  Median:                      {np.median(residuals):.6f}\")\n",
    "print(f\"  Std Deviation:               {residuals.std():.6f}\")\n",
    "print(f\"  Min Error:                   {residuals.min():.6f}\")\n",
    "print(f\"  Max Error:                   {residuals.max():.6f}\")\n",
    "print(f\"  Skewness:                    {skew:.4f}\")\n",
    "print(f\"  Kurtosis:                    {kurt:.4f}\")\n",
    "\n",
    "print(f\"\\nError Distribution:\")\n",
    "print(f\"  % predictions within 1 MAE:  {np.sum(np.abs(residuals) <= mae) / len(residuals) * 100:.2f}%\")\n",
    "print(f\"  % predictions within 2 MAE:  {np.sum(np.abs(residuals) <= 2*mae) / len(residuals) * 100:.2f}%\")\n",
    "print(f\"  % predictions within 1σ:     {np.sum(np.abs(residuals) <= std_residuals) / len(residuals) * 100:.2f}%\")\n",
    "print(f\"  % predictions within 2σ:     {np.sum(np.abs(residuals) <= 2*std_residuals) / len(residuals) * 100:.2f}%\")\n",
    "\n",
    "print(f\"\\nPrediction Quality by Quartile:\")\n",
    "for i, q in enumerate(quartiles.unique()):\n",
    "    print(f\"  {q}: MAE = {quartile_mae[i]:.6f}, R² = {quartile_r2[i]:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ALL PLOTS SAVED SUCCESSFULLY!\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nGenerated files in 'figures/' directory:\")\n",
    "for i in range(1, 13):\n",
    "    print(f\"  {i:02d}_*.png\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf7fea5b-612e-4978-b572-381f916b742d",
   "metadata": {},
   "source": [
    "## Directional Accuracy Analysis\n",
    "\n",
    "### Purpose\n",
    "Evaluate the model's ability to predict the direction of volatility changes (increase vs. decrease), not just the magnitude. This is critical for trading applications where knowing which way volatility will move matters more than the exact value.\n",
    "\n",
    "### Key Insight: Direction vs. Magnitude\n",
    "\n",
    "While the model achieves strong R² (~57%) for predicting volatility **levels**, directional accuracy is a separate challenge:\n",
    "- **High R²** = Good at predicting \"how much\" volatility\n",
    "- **Directional accuracy** = Good at predicting \"which way\" volatility moves\n",
    "\n",
    "A model can have high R² but poor directional accuracy if it correctly estimates magnitudes but frequently misses turning points.\n",
    "\n",
    "### Three Evaluation Methods\n",
    "\n",
    "**Method 1: Sequential Forecast Changes**\n",
    "- Compares consecutive 10-day forecasts: \"Will tomorrow's forecast be higher than today's?\"\n",
    "- Accounts for the 9-day overlap in predictions (day 1-10 vs. day 2-11)\n",
    "- Measures incremental forecast revision accuracy\n",
    "- Most relevant for daily forecast updates\n",
    "\n",
    "**Method 2: Binary Classification (Up/Down)**\n",
    "- Treats direction prediction as a classification problem\n",
    "- **Confusion Matrix** shows:\n",
    "  - True Positives: Correctly predicted increases\n",
    "  - True Negatives: Correctly predicted decreases\n",
    "  - False Positives: Predicted increase, actually decreased (Type I error)\n",
    "  - False Negatives: Predicted decrease, actually increased (Type II error)\n",
    "- Includes precision, recall, and F1-scores for each direction\n",
    "- Benchmark: 50% (random coin flip)\n",
    "\n",
    "**Method 3: Three-Class (Up/Down/Flat)**\n",
    "- Adds \"flat\" category for very small changes (<1% of volatility std dev)\n",
    "- More realistic: not every change is meaningful\n",
    "- Harder task but more useful for practical applications\n",
    "- Avoids penalizing the model for missing noise\n",
    "\n",
    "### Why Directional Accuracy Might Be Low\n",
    "\n",
    "**Overlapping Windows**: 10-day forecasts share 9 days, so consecutive changes are small and noisy\n",
    "\n",
    "**Temporal Lag**: LSTMs often predict values close to the last observation, causing lag at turning points\n",
    "\n",
    "**Regime Changes**: Volatility direction shifts are inherently difficult to predict—even humans struggle\n",
    "\n",
    "**Optimization Mismatch**: Model was trained to minimize MSE (magnitude error), not maximize directional accuracy\n",
    "\n",
    "### Diagnostic Plots\n",
    "\n",
    "**1. Confusion Matrix Heatmap**\n",
    "- Visual breakdown of correct/incorrect directional predictions\n",
    "- Shows if model is biased (e.g., over-predicting increases)\n",
    "\n",
    "**2. Accuracy by Change Magnitude**\n",
    "- Splits predictions into small/medium/large changes\n",
    "- Tests hypothesis: \"Does model perform better on significant moves?\"\n",
    "- Shows sample size for each category\n",
    "\n",
    "**3. Actual vs. Predicted Changes Scatter**\n",
    "- Quadrant analysis:\n",
    "  - Green quadrants (top-right, bottom-left) = correct direction\n",
    "  - Red quadrants (top-left, bottom-right) = wrong direction\n",
    "- Distance from perfect prediction line shows magnitude error\n",
    "- Clustering near axes = model hesitant to predict large changes\n",
    "\n",
    "**4. Rolling Directional Accuracy**\n",
    "- 50-period moving average smooths short-term noise\n",
    "- Identifies periods where model performs better/worse\n",
    "- Compares to random chance (50%) and overall average\n",
    "\n",
    "**5. Change Magnitude Distribution**\n",
    "- Overlapping histograms: correct vs. incorrect predictions\n",
    "- Tests if errors occur more on small or large changes\n",
    "- Mean comparison shows if model struggles with certain volatility regimes\n",
    "\n",
    "### Interpretation Guidelines\n",
    "\n",
    "**Good Directional Performance:**\n",
    "- Accuracy consistently >55% (significant edge over random)\n",
    "- Balanced precision/recall for both increases and decreases\n",
    "- Better accuracy on large changes (meaningful predictions)\n",
    "- Stable rolling accuracy (not regime-dependent)\n",
    "\n",
    "**Warning Signs:**\n",
    "- ~50% accuracy = model not capturing directional information\n",
    "- High bias (predicting one direction far more than it occurs)\n",
    "- Worse on large changes = missing the most important signals\n",
    "- Deteriorating rolling accuracy = concept drift over time\n",
    "\n",
    "### Practical Implications\n",
    "\n",
    "**For Trading:**\n",
    "- Directional accuracy >55% can be profitable with proper risk management\n",
    "- Even 52-53% edge is valuable for position sizing decisions\n",
    "- Combine with magnitude predictions for optimal entry/exit timing\n",
    "\n",
    "**For Risk Management:**\n",
    "- Focus on magnitude predictions (where model excels)\n",
    "- Use directional signal with caution or as supplementary information\n",
    "- Consider regime-based strategies (model performs better in certain volatility regimes)\n",
    "\n",
    "### Alternative Approaches\n",
    "\n",
    "If directional accuracy is critical:\n",
    "1. **Train separate classifier** specifically for direction using binary cross-entropy loss\n",
    "2. **Add directional loss term** to training objective (multi-task learning)\n",
    "3. **Use non-overlapping windows** (every 10th prediction) for cleaner signals\n",
    "4. **Engineer momentum features** (RSI, MACD) to capture trend information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "55a77435-024f-43c4-a6c2-91fb95516da2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "146/146 [==============================] - 0s 1ms/step\n",
      "======================================================================\n",
      "DIRECTIONAL ACCURACY ANALYSIS\n",
      "======================================================================\n",
      "\n",
      "Method 1: Direction relative to previous actual value\n",
      "Directional Accuracy: 52.18%\n",
      "Correctly predicted direction: 2431 out of 4659\n",
      "Accuracy predicting INCREASES: 60.25% (1396/2317)\n",
      "Accuracy predicting DECREASES: 44.19% (1035/2342)\n",
      "\n",
      "======================================================================\n",
      "Method 2: Binary Classification (Increase vs Decrease)\n",
      "======================================================================\n",
      "\n",
      "Binary Directional Accuracy: 52.18%\n",
      "\n",
      "Confusion Matrix:\n",
      "                 Predicted Down  Predicted Up\n",
      "Actual Down            1035          1307\n",
      "Actual Up               921          1396\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Decrease     0.5291    0.4419    0.4816      2342\n",
      "    Increase     0.5165    0.6025    0.5562      2317\n",
      "\n",
      "    accuracy                         0.5218      4659\n",
      "   macro avg     0.5228    0.5222    0.5189      4659\n",
      "weighted avg     0.5228    0.5218    0.5187      4659\n",
      "\n",
      "\n",
      "======================================================================\n",
      "Method 3: Three-Class Classification (Increase/Decrease/Flat)\n",
      "======================================================================\n",
      "\n",
      "Three-Class Accuracy: 41.51%\n",
      "Increases: 58.84% (1078/1832)\n",
      "Decreases: 45.39% (837/1844)\n",
      "Flat: 1.93% (19/983)\n",
      "\n",
      "======================================================================\n",
      "GENERATING VISUALIZATIONS...\n",
      "======================================================================\n",
      "\n",
      "1. Creating confusion matrix heatmap...\n",
      "2. Creating accuracy by change magnitude...\n",
      "3. Creating actual vs predicted changes scatter...\n",
      "4. Creating rolling directional accuracy...\n",
      "5. Creating change magnitude distribution...\n",
      "\n",
      "======================================================================\n",
      "SUMMARY STATISTICS\n",
      "======================================================================\n",
      "Total predictions analyzed: 4659\n",
      "Overall directional accuracy: 52.18%\n",
      "Better than random (50%): YES\n",
      "Improvement over random: 2.18 percentage points\n",
      "\n",
      "Actual volatility increases: 2317 (49.7%)\n",
      "Actual volatility decreases: 2342 (50.3%)\n",
      "Predicted increases: 2703 (58.0%)\n",
      "Predicted decreases: 1956 (42.0%)\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "ALL DIRECTIONAL ACCURACY PLOTS SAVED!\n",
      "======================================================================\n",
      "\n",
      "Generated files in 'figures/' directory:\n",
      "  directional_01_confusion_matrix.png\n",
      "  directional_02_accuracy_by_magnitude.png\n",
      "  directional_03_changes_scatter.png\n",
      "  directional_04_rolling_accuracy.png\n",
      "  directional_05_magnitude_distribution.png\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "### Directional volatility change analysis and plots\n",
    "\n",
    "# Load the best model\n",
    "best_model = load_model('best_lstm_model.keras')\n",
    "\n",
    "# Prepare test data\n",
    "X_test_reshaped = X_test.values.reshape(-1, X_test.shape[1], 1)\n",
    "\n",
    "# Get predictions\n",
    "y_pred = best_model.predict(X_test_reshaped).flatten()\n",
    "\n",
    "# Convert to actual values\n",
    "y_true = y_test.values\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"DIRECTIONAL ACCURACY ANALYSIS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Calculate the direction (sign) of changes\n",
    "# For the first prediction, we compare against the last training value\n",
    "# For subsequent predictions, we compare against the previous actual value\n",
    "\n",
    "# Method 1: Direction relative to previous actual value\n",
    "if len(y_true) > 1:\n",
    "    # Calculate actual changes (direction)\n",
    "    actual_changes = np.diff(y_true)\n",
    "    actual_direction = np.sign(actual_changes)  # 1 for increase, -1 for decrease, 0 for no change\n",
    "    \n",
    "    # Calculate predicted changes\n",
    "    # Compare prediction to previous actual value\n",
    "    predicted_changes = y_pred[1:] - y_true[:-1]\n",
    "    predicted_direction = np.sign(predicted_changes)\n",
    "    \n",
    "    # Calculate directional accuracy\n",
    "    directional_matches = (actual_direction == predicted_direction)\n",
    "    directional_accuracy = np.mean(directional_matches) * 100\n",
    "    \n",
    "    print(f\"\\nMethod 1: Direction relative to previous actual value\")\n",
    "    print(f\"Directional Accuracy: {directional_accuracy:.2f}%\")\n",
    "    print(f\"Correctly predicted direction: {np.sum(directional_matches)} out of {len(directional_matches)}\")\n",
    "    \n",
    "    # Separate analysis for increases vs decreases\n",
    "    increase_mask = actual_direction > 0\n",
    "    decrease_mask = actual_direction < 0\n",
    "    \n",
    "    if np.sum(increase_mask) > 0:\n",
    "        increase_accuracy = np.mean(directional_matches[increase_mask]) * 100\n",
    "        print(f\"Accuracy predicting INCREASES: {increase_accuracy:.2f}% ({np.sum(directional_matches[increase_mask])}/{np.sum(increase_mask)})\")\n",
    "    \n",
    "    if np.sum(decrease_mask) > 0:\n",
    "        decrease_accuracy = np.mean(directional_matches[decrease_mask]) * 100\n",
    "        print(f\"Accuracy predicting DECREASES: {decrease_accuracy:.2f}% ({np.sum(directional_matches[decrease_mask])}/{np.sum(decrease_mask)})\")\n",
    "\n",
    "# Method 2: Binary classification - Up or Down\n",
    "print(f\"\\n\" + \"=\"*70)\n",
    "print(\"Method 2: Binary Classification (Increase vs Decrease)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Calculate changes from previous value\n",
    "actual_changes_full = np.diff(y_true)\n",
    "predicted_changes_full = y_pred[1:] - y_true[:-1]\n",
    "\n",
    "# Binary: 1 for increase, 0 for decrease\n",
    "actual_binary = (actual_changes_full > 0).astype(int)\n",
    "predicted_binary = (predicted_changes_full > 0).astype(int)\n",
    "\n",
    "# Calculate metrics\n",
    "binary_accuracy = accuracy_score(actual_binary, predicted_binary) * 100\n",
    "print(f\"\\nBinary Directional Accuracy: {binary_accuracy:.2f}%\")\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(actual_binary, predicted_binary)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(\"                 Predicted Down  Predicted Up\")\n",
    "print(f\"Actual Down          {cm[0,0]:6d}        {cm[0,1]:6d}\")\n",
    "print(f\"Actual Up            {cm[1,0]:6d}        {cm[1,1]:6d}\")\n",
    "\n",
    "# Classification Report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(actual_binary, predicted_binary, \n",
    "                          target_names=['Decrease', 'Increase'],\n",
    "                          digits=4))\n",
    "\n",
    "# Method 3: Three-class classification (Up, Down, Flat)\n",
    "print(f\"\\n\" + \"=\"*70)\n",
    "print(\"Method 3: Three-Class Classification (Increase/Decrease/Flat)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Define threshold for \"flat\" (e.g., changes less than 1% of std)\n",
    "threshold = 0.01 * np.std(y_true)\n",
    "\n",
    "# Three-class: 1 for increase, -1 for decrease, 0 for flat\n",
    "actual_three_class = np.sign(actual_changes_full)\n",
    "actual_three_class[np.abs(actual_changes_full) < threshold] = 0\n",
    "\n",
    "predicted_three_class = np.sign(predicted_changes_full)\n",
    "predicted_three_class[np.abs(predicted_changes_full) < threshold] = 0\n",
    "\n",
    "three_class_accuracy = np.mean(actual_three_class == predicted_three_class) * 100\n",
    "print(f\"\\nThree-Class Accuracy: {three_class_accuracy:.2f}%\")\n",
    "\n",
    "# Detailed breakdown\n",
    "for cls, name in [(1, 'Increases'), (-1, 'Decreases'), (0, 'Flat')]:\n",
    "    mask = actual_three_class == cls\n",
    "    if np.sum(mask) > 0:\n",
    "        cls_accuracy = np.mean((actual_three_class == predicted_three_class)[mask]) * 100\n",
    "        print(f\"{name}: {cls_accuracy:.2f}% ({np.sum((actual_three_class == predicted_three_class)[mask])}/{np.sum(mask)})\")\n",
    "\n",
    "# Visualization\n",
    "print(f\"\\n\" + \"=\"*70)\n",
    "print(\"GENERATING VISUALIZATIONS...\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Split by change magnitude\n",
    "change_magnitudes = np.abs(actual_changes)\n",
    "small_threshold = np.percentile(change_magnitudes, 33)\n",
    "large_threshold = np.percentile(change_magnitudes, 67)\n",
    "\n",
    "small_moves = change_magnitudes < small_threshold\n",
    "medium_moves = (change_magnitudes >= small_threshold) & (change_magnitudes < large_threshold)\n",
    "large_moves = change_magnitudes >= large_threshold\n",
    "\n",
    "# ============================================================================\n",
    "# Plot 1: Confusion Matrix Heatmap\n",
    "# ============================================================================\n",
    "print(\"\\n1. Creating confusion matrix heatmap...\")\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax,\n",
    "            xticklabels=['Down', 'Up'], yticklabels=['Down', 'Up'],\n",
    "            cbar_kws={'label': 'Count'}, annot_kws={'size': 16})\n",
    "ax.set_title('Confusion Matrix: Directional Predictions', fontsize=16, fontweight='bold', pad=20)\n",
    "ax.set_ylabel('Actual Direction', fontsize=13)\n",
    "ax.set_xlabel('Predicted Direction', fontsize=13)\n",
    "\n",
    "# Add accuracy text\n",
    "textstr = f'Overall Accuracy: {binary_accuracy:.2f}%'\n",
    "props = dict(boxstyle='round', facecolor='wheat', alpha=0.8)\n",
    "ax.text(0.98, 0.02, textstr, transform=ax.transAxes, fontsize=12,\n",
    "        verticalalignment='bottom', horizontalalignment='right', bbox=props)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('figures/directional_01_confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "# ============================================================================\n",
    "# Plot 2: Directional Accuracy by Change Magnitude\n",
    "# ============================================================================\n",
    "print(\"2. Creating accuracy by change magnitude...\")\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "accuracies = [np.mean(directional_matches[small_moves])*100,\n",
    "              np.mean(directional_matches[medium_moves])*100,\n",
    "              np.mean(directional_matches[large_moves])*100]\n",
    "labels = ['Small\\nChanges', 'Medium\\nChanges', 'Large\\nChanges']\n",
    "colors = ['lightblue', 'skyblue', 'steelblue']\n",
    "\n",
    "bars = ax.bar(labels, accuracies, color=colors, edgecolor='black', alpha=0.8, width=0.6)\n",
    "ax.axhline(y=50, color='red', linestyle='--', linewidth=2, label='Random chance (50%)')\n",
    "ax.set_ylabel('Directional Accuracy (%)', fontsize=12)\n",
    "ax.set_title('Directional Accuracy by Change Magnitude', fontsize=16, fontweight='bold', pad=20)\n",
    "ax.set_ylim([0, 100])\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, acc in zip(bars, accuracies):\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., height + 2,\n",
    "            f'{acc:.1f}%', ha='center', va='bottom', fontsize=12, fontweight='bold')\n",
    "\n",
    "# Add count labels\n",
    "counts = [np.sum(small_moves), np.sum(medium_moves), np.sum(large_moves)]\n",
    "for bar, count in zip(bars, counts):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., 5,\n",
    "            f'n={count}', ha='center', va='bottom', fontsize=10, style='italic')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('figures/directional_02_accuracy_by_magnitude.png', dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "# ============================================================================\n",
    "# Plot 3: Actual vs Predicted Changes\n",
    "# ============================================================================\n",
    "print(\"3. Creating actual vs predicted changes scatter...\")\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "ax.scatter(actual_changes_full, predicted_changes_full, alpha=0.5, s=30, edgecolors='k', linewidth=0.5)\n",
    "ax.axhline(y=0, color='k', linestyle='-', linewidth=1)\n",
    "ax.axvline(x=0, color='k', linestyle='-', linewidth=1)\n",
    "\n",
    "# Perfect prediction line\n",
    "max_abs = max(abs(actual_changes_full.max()), abs(actual_changes_full.min()),\n",
    "              abs(predicted_changes_full.max()), abs(predicted_changes_full.min()))\n",
    "ax.plot([-max_abs, max_abs], [-max_abs, max_abs], 'r--', linewidth=2.5, label='Perfect prediction')\n",
    "\n",
    "ax.set_title('Actual vs Predicted Changes', fontsize=16, fontweight='bold', pad=20)\n",
    "ax.set_xlabel('Actual Change in Volatility', fontsize=12)\n",
    "ax.set_ylabel('Predicted Change in Volatility', fontsize=12)\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Highlight quadrants\n",
    "ax.fill_between([0, max_abs], 0, max_abs, alpha=0.1, color='green', label='_nolegend_')  # Top-right\n",
    "ax.fill_between([-max_abs, 0], -max_abs, 0, alpha=0.1, color='green', label='_nolegend_')  # Bottom-left\n",
    "ax.fill_between([0, max_abs], -max_abs, 0, alpha=0.1, color='red', label='_nolegend_')  # Bottom-right\n",
    "ax.fill_between([-max_abs, 0], 0, max_abs, alpha=0.1, color='red', label='_nolegend_')  # Top-left\n",
    "\n",
    "# Add quadrant labels\n",
    "ax.text(0.7, 0.95, 'Correct\\nDirection', transform=ax.transAxes, fontsize=10,\n",
    "        verticalalignment='top', horizontalalignment='center',\n",
    "        bbox=dict(boxstyle='round', facecolor='lightgreen', alpha=0.7))\n",
    "ax.text(0.3, 0.05, 'Correct\\nDirection', transform=ax.transAxes, fontsize=10,\n",
    "        verticalalignment='bottom', horizontalalignment='center',\n",
    "        bbox=dict(boxstyle='round', facecolor='lightgreen', alpha=0.7))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('figures/directional_03_changes_scatter.png', dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "# ============================================================================\n",
    "# Plot 4: Rolling Directional Accuracy\n",
    "# ============================================================================\n",
    "print(\"4. Creating rolling directional accuracy...\")\n",
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "window = 50\n",
    "rolling_accuracy = pd.Series(directional_matches.astype(int)).rolling(window=window).mean() * 100\n",
    "ax.plot(rolling_accuracy, linewidth=2.5, color='steelblue')\n",
    "ax.axhline(y=50, color='red', linestyle='--', linewidth=2, label='Random chance (50%)')\n",
    "ax.axhline(y=directional_accuracy, color='green', linestyle='--', linewidth=2,\n",
    "           label=f'Overall accuracy ({directional_accuracy:.1f}%)')\n",
    "ax.set_title(f'Rolling Directional Accuracy (window={window})', fontsize=16, fontweight='bold', pad=20)\n",
    "ax.set_xlabel('Time Step', fontsize=12)\n",
    "ax.set_ylabel('Accuracy (%)', fontsize=12)\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_ylim([0, 100])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('figures/directional_04_rolling_accuracy.png', dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "# ============================================================================\n",
    "# Plot 5: Change Magnitude Distribution (Correct vs Incorrect)\n",
    "# ============================================================================\n",
    "print(\"5. Creating change magnitude distribution...\")\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "correct_direction = np.abs(actual_changes[directional_matches])\n",
    "incorrect_direction = np.abs(actual_changes[~directional_matches])\n",
    "\n",
    "ax.hist([correct_direction, incorrect_direction], \n",
    "        bins=40, label=['Correct Direction', 'Incorrect Direction'],\n",
    "        alpha=0.7, edgecolor='black', color=['green', 'red'])\n",
    "ax.set_xlabel('Magnitude of Actual Change', fontsize=12)\n",
    "ax.set_ylabel('Frequency', fontsize=12)\n",
    "ax.set_title('Change Magnitude: Correct vs Incorrect Predictions', fontsize=16, fontweight='bold', pad=20)\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add statistics\n",
    "textstr = f'Correct mean: {np.mean(correct_direction):.6f}\\nIncorrect mean: {np.mean(incorrect_direction):.6f}'\n",
    "props = dict(boxstyle='round', facecolor='wheat', alpha=0.8)\n",
    "ax.text(0.98, 0.97, textstr, transform=ax.transAxes, fontsize=10,\n",
    "        verticalalignment='top', horizontalalignment='right', bbox=props)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('figures/directional_05_magnitude_distribution.png', dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "# Summary Statistics\n",
    "print(f\"\\n\" + \"=\"*70)\n",
    "print(\"SUMMARY STATISTICS\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Total predictions analyzed: {len(directional_matches)}\")\n",
    "print(f\"Overall directional accuracy: {directional_accuracy:.2f}%\")\n",
    "print(f\"Better than random (50%): {'YES' if directional_accuracy > 50 else 'NO'}\")\n",
    "print(f\"Improvement over random: {directional_accuracy - 50:.2f} percentage points\")\n",
    "print(f\"\\nActual volatility increases: {np.sum(actual_binary)} ({np.sum(actual_binary)/len(actual_binary)*100:.1f}%)\")\n",
    "print(f\"Actual volatility decreases: {np.sum(1-actual_binary)} ({np.sum(1-actual_binary)/len(actual_binary)*100:.1f}%)\")\n",
    "print(f\"Predicted increases: {np.sum(predicted_binary)} ({np.sum(predicted_binary)/len(predicted_binary)*100:.1f}%)\")\n",
    "print(f\"Predicted decreases: {np.sum(1-predicted_binary)} ({np.sum(1-predicted_binary)/len(predicted_binary)*100:.1f}%)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ALL DIRECTIONAL ACCURACY PLOTS SAVED!\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nGenerated files in 'figures/' directory:\")\n",
    "print(\"  directional_01_confusion_matrix.png\")\n",
    "print(\"  directional_02_accuracy_by_magnitude.png\")\n",
    "print(\"  directional_03_changes_scatter.png\")\n",
    "print(\"  directional_04_rolling_accuracy.png\")\n",
    "print(\"  directional_05_magnitude_distribution.png\")\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
