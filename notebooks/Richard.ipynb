{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe350561-04ba-4818-9cfd-c30a8d611a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Dependencies\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb460460-ab85-4639-8176-f9175d1cbf8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_85901/3369933274.py:11: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df = pd.read_csv(data_path, parse_dates=['Date'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Return  High_Low_pct  Close_Open_pct       MA5      MA10  \\\n",
      "1  0.023311      -0.92304       -0.023226 -0.647124 -0.647122   \n",
      "2 -0.217569      -0.92304       -0.023226 -0.647148 -0.647185   \n",
      "3  0.554534      -0.92304       -0.023226 -0.647173 -0.647209   \n",
      "4  0.454640      -0.92304       -0.023226 -0.647201 -0.647234   \n",
      "5  0.737950      -0.92304       -0.023226 -0.647104 -0.647214   \n",
      "\n",
      "   Volume_pct_change  Volume_MA5  Return_lag1  Return_lag2  Volatility_lag1  \n",
      "1          -0.060194   -0.522899    -1.399673     0.499832        -0.166255  \n",
      "2          -0.060194   -0.522899     0.023281    -1.399676        -0.217933  \n",
      "3          -0.060194   -0.522899    -0.217598     0.023273        -0.217969  \n",
      "4          -0.060194   -0.522899     0.554501    -0.217605        -0.203854  \n",
      "5          -0.060194   -0.522899     0.454607     0.554491        -0.210628   1    0.007275\n",
      "2    0.007273\n",
      "3    0.007181\n",
      "4    0.008303\n",
      "5    0.007715\n",
      "Name: Volatility_future, dtype: float64\n",
      "Train size: 18642, Test size: 4660\n"
     ]
    }
   ],
   "source": [
    "### Data Loading\n",
    "\n",
    "def load_csv(file_name: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load a CSV file from the data directory.\n",
    "    \"\"\"\n",
    "    data_path = Path.cwd().parent / \"data\" / file_name \n",
    "    if not data_path.exists():\n",
    "        raise FileNotFoundError(f\"{data_path} not found.\")\n",
    "\n",
    "    df = pd.read_csv(data_path, parse_dates=['Date'])\n",
    "    \n",
    "    # Parse 'Date' column explicitly\n",
    "    # MM/DD/YY format\n",
    "    df['Date'] = pd.to_datetime(df['Date'], format='%m/%d/%y', errors='coerce')\n",
    "    \n",
    "    # Correct future years (pandas may interpret '30' as 2030)\n",
    "    # Assume dates from 1928â€“2020\n",
    "    df.loc[df['Date'] > pd.Timestamp.today(), 'Date'] -= pd.offsets.DateOffset(years=100)\n",
    "    \n",
    "    # Sort by date\n",
    "    df = df.sort_values('Date').reset_index(drop=True)\n",
    "    return df \n",
    "\n",
    "\n",
    "\n",
    "def add_features(df: pd.DataFrame, rolling_window: int = 10):\n",
    "    \"\"\"\n",
    "    Add features for ML: returns, rolling volatility, price/volume features, lags.\n",
    "    Handles NaN and infinite values, ready for scaling.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Returns and volatility\n",
    "    df['Return'] = df['Adj Close'].pct_change()\n",
    "    df['Volatility'] = df['Return'].rolling(rolling_window).std()\n",
    "\n",
    "    # Price-based features\n",
    "    df['High_Low_pct'] = (df['High'] - df['Low']) / df['Close']\n",
    "    df['Close_Open_pct'] = (df['Close'] - df['Open']) / df['Open']\n",
    "    df['MA5'] = df['Close'].rolling(5).mean()\n",
    "    df['MA10'] = df['Close'].rolling(10).mean()\n",
    "\n",
    "    # Volume-based features\n",
    "    df['Volume_pct_change'] = df['Volume'].pct_change()\n",
    "    df['Volume_MA5'] = df['Volume'].rolling(5).mean()\n",
    "\n",
    "    # Lag features\n",
    "    df['Return_lag1'] = df['Return'].shift(1)\n",
    "    df['Return_lag2'] = df['Return'].shift(2)\n",
    "    df['Volatility_lag1'] = df['Volatility'].shift(1)\n",
    "\n",
    "    # Target: 10 days-ahead volatility\n",
    "    df['Volatility_future'] = df['Volatility'].shift(-10)\n",
    "\n",
    "    # Replace inf with NaN\n",
    "    df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "    # Fill volume-related NaNs (common in early data)\n",
    "    df['Volume_pct_change'] = df['Volume_pct_change'].fillna(0)\n",
    "    df['Volume_MA5'] = df['Volume_MA5'].ffill()\n",
    "\n",
    "    # Drop only rows where the key features are missing\n",
    "    df = df[df['Volatility'].notna() & df['Volatility_future'].notna()]\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # Features to scale\n",
    "    features = [\n",
    "        'Return', 'High_Low_pct', 'Close_Open_pct', 'MA5', 'MA10',\n",
    "        'Volume_pct_change', 'Volume_MA5', 'Return_lag1', 'Return_lag2', 'Volatility_lag1'\n",
    "    ]\n",
    "\n",
    "    # Scaling\n",
    "    scaler = StandardScaler()\n",
    "    df[features] = scaler.fit_transform(df[features].values)\n",
    "\n",
    "    target_col = 'Volatility_future'\n",
    "    return df, features, target_col\n",
    "\n",
    "\n",
    "# Load raw data\n",
    "df = load_csv(\"SPX.csv\")\n",
    "\n",
    "# Preprocess\n",
    "df_prepared, feature_cols, target_col = add_features(df)\n",
    "\n",
    "X = df_prepared[feature_cols]\n",
    "y = df_prepared[target_col]\n",
    "\n",
    "# Drop any row where X or y has NaN\n",
    "mask = (~X.isna().any(axis=1)) & (~y.isna())\n",
    "X = X[mask]\n",
    "y = y[mask]\n",
    "\n",
    "print(X.head(), y.head())\n",
    "\n",
    "\n",
    "# Split data into train (80%) and test (20%) by time\n",
    "split_idx = int(len(df_prepared) * 0.8)\n",
    "\n",
    "X_train, X_test = X.iloc[:split_idx], X.iloc[split_idx:]\n",
    "y_train, y_test = y.iloc[:split_idx], y.iloc[split_idx:]\n",
    "\n",
    "print(f\"Train size: {len(X_train)}, Test size: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9140cdad-a72a-48b1-9f80-f5e8dc7f99c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 0.000036\n",
      "Accuracy (R2): 40.08%\n",
      "-------------------\n",
      "Sample predictions:\n",
      "[0.01309724 0.01442836 0.01458687 0.01409432 0.01318505]\n"
     ]
    }
   ],
   "source": [
    "# Train Linear Regression Model (Richard)\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Initialize and fit the model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate performance\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# Print results\n",
    "print(f\"Mean Squared Error: {mse:.6f}\")\n",
    "print(f\"Accuracy (R2): {r2 * 100:.2f}%\")\n",
    "\n",
    "# First predictions\n",
    "print(\"-------------------\")\n",
    "print(\"Sample predictions:\")\n",
    "print(y_pred[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af35784-3e58-49ff-9233-54a441864c9d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
