{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe350561-04ba-4818-9cfd-c30a8d611a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Dependencies\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bb460460-ab85-4639-8176-f9175d1cbf8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Return  High_Low_pct  Close_Open_pct       MA5      MA10  \\\n",
      "0 -0.142291      0.565622       -0.190756 -0.686450 -0.686529   \n",
      "1 -0.292893      0.426730       -0.399186 -0.686127 -0.686630   \n",
      "2 -0.071371      0.344870       -0.092604 -0.685998 -0.686577   \n",
      "3  0.254500      0.308508        0.358396 -0.685905 -0.686425   \n",
      "4  0.332750      0.455867        0.466694 -0.685840 -0.686268   \n",
      "\n",
      "   Volume_pct_change  Volume_MA5  Return_lag1  Return_lag2  Volatility_lag1  \n",
      "0          -0.475246   -0.618562     0.522312     0.455054        -0.131453  \n",
      "1          -0.261197   -0.618588    -0.183880     0.522396        -0.138714  \n",
      "2          -0.081415   -0.618562    -0.372534    -0.183897        -0.395098  \n",
      "3           0.113468   -0.618603    -0.095040    -0.372578        -0.670212  \n",
      "4           0.435448   -0.618601     0.313170    -0.095044        -0.710004   0    0.007430\n",
      "1    0.005999\n",
      "2    0.004463\n",
      "3    0.004241\n",
      "4    0.004261\n",
      "Name: Volatility, dtype: float64\n",
      "Train size: 14252, Test size: 3564\n"
     ]
    }
   ],
   "source": [
    "### Data Loading\n",
    "\n",
    "def load_csv(file_name: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load a CSV file from the data directory.\n",
    "    \"\"\"\n",
    "    data_path = Path.cwd().parent / \"data\" / file_name \n",
    "    if not data_path.exists():\n",
    "        raise FileNotFoundError(f\"{data_path} not found.\")\n",
    "\n",
    "    df = pd.read_csv(data_path, parse_dates=['Date'])\n",
    "    \n",
    "    # Parse 'Date' column explicitly\n",
    "    # MM/DD/YY format\n",
    "    df['Date'] = pd.to_datetime(df['Date'], format='%m/%d/%y', errors='coerce')\n",
    "    \n",
    "    # Correct future years (pandas may interpret '30' as 2030)\n",
    "    # Assume dates from 1928â€“2020\n",
    "    df.loc[df['Date'] > pd.Timestamp.today(), 'Date'] -= pd.offsets.DateOffset(years=100)\n",
    "    \n",
    "    # Sort by date\n",
    "    df = df.sort_values('Date').reset_index(drop=True)\n",
    "    return df \n",
    "\n",
    "\n",
    "\n",
    "def add_features(df: pd.DataFrame, rolling_window: int = 10):\n",
    "    \"\"\"\n",
    "    Add features for ML: returns, rolling volatility, price/volume features, lags.\n",
    "    Handles NaN and infinite values, ready for scaling.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Returns and volatility\n",
    "    df['Return'] = df['Adj Close'].pct_change()\n",
    "    df['Volatility'] = df['Return'].rolling(rolling_window).std()\n",
    "\n",
    "    # Price-based features\n",
    "    df['High_Low_pct'] = (df['High'] - df['Low']) / df['Close']\n",
    "    df['Close_Open_pct'] = (df['Close'] - df['Open']) / df['Open']\n",
    "    df['MA5'] = df['Close'].rolling(5).mean()\n",
    "    df['MA10'] = df['Close'].rolling(10).mean()\n",
    "\n",
    "    # Volume-based features\n",
    "    df['Volume_pct_change'] = df['Volume'].pct_change()\n",
    "    df['Volume_MA5'] = df['Volume'].rolling(5).mean()\n",
    "\n",
    "    # Lag features\n",
    "    df['Return_lag1'] = df['Return'].shift(1)\n",
    "    df['Return_lag2'] = df['Return'].shift(2)\n",
    "    df['Volatility_lag1'] = df['Volatility'].shift(1)\n",
    "\n",
    "    # Target: 10 days-ahead volatility\n",
    "    df['Volatility_future'] = df['Volatility'].shift(-10)\n",
    "\n",
    "    # Replace inf with NaN\n",
    "    df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "    # Fill volume-related NaNs (common in early data)\n",
    "    df['Volume_pct_change'] = df['Volume_pct_change'].fillna(0)\n",
    "    df['Volume_MA5'] = df['Volume_MA5'].ffill()\n",
    "\n",
    "    # Drop only rows where the key features are missing\n",
    "    df = df[df['Volatility'].notna() & df['Volatility_future'].notna()]\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # Features to scale\n",
    "    features = [\n",
    "        'Return', 'High_Low_pct', 'Close_Open_pct', 'MA5', 'MA10',\n",
    "        'Volume_pct_change', 'Volume_MA5', 'Return_lag1', 'Return_lag2', 'Volatility_lag1'\n",
    "    ]\n",
    "\n",
    "    # Scaling\n",
    "    scaler = StandardScaler()\n",
    "    df[features] = scaler.fit_transform(df[features].values)\n",
    "\n",
    "    target_col = 'Volatility_future'\n",
    "    return df, features, target_col\n",
    "\n",
    "\n",
    "# Load raw data\n",
    "df = load_csv(\"SPX.csv\")\n",
    "\n",
    "# Preprocess\n",
    "df_prepared, feature_cols, target_col = add_features(df)\n",
    "\n",
    "X = df_prepared[feature_cols]\n",
    "y = df_prepared[target_col]\n",
    "\n",
    "# Drop any row where X or y has NaN\n",
    "mask = (~X.isna().any(axis=1)) & (~y.isna())\n",
    "X = X[mask]\n",
    "y = y[mask]\n",
    "\n",
    "print(X.head(), y.head())\n",
    "\n",
    "\n",
    "# Split data into train (80%) and test (20%) by time\n",
    "split_idx = int(len(df_prepared) * 0.8)\n",
    "\n",
    "X_train, X_test = X.iloc[:split_idx], X.iloc[split_idx:]\n",
    "y_train, y_test = y.iloc[:split_idx], y.iloc[split_idx:]\n",
    "\n",
    "print(f\"Train size: {len(X_train)}, Test size: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9140cdad-a72a-48b1-9f80-f5e8dc7f99c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 0.000003\n",
      "Accuracy (R2): 71.25%\n",
      "-------------------\n",
      "Sample predictions:\n",
      "[0.0048275  0.00585225 0.00520611 0.00334023 0.00316915]\n"
     ]
    }
   ],
   "source": [
    "# Train Linear Regression Model (Richard)\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Initialize and fit the model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate performance\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# Print results\n",
    "print(f\"Mean Squared Error: {mse:.6f}\")\n",
    "print(f\"Accuracy (R2): {r2 * 100:.2f}%\")\n",
    "\n",
    "# First predictions\n",
    "print(\"-------------------\")\n",
    "print(\"Sample predictions:\")\n",
    "print(y_pred[:5])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
