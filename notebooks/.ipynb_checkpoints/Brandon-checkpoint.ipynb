{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe350561-04ba-4818-9cfd-c30a8d611a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Dependencies\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb460460-ab85-4639-8176-f9175d1cbf8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1281/2251304966.py:11: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df = pd.read_csv(data_path, parse_dates=['Date'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Return  High_Low_pct  Close_Open_pct       MA5      MA10  \\\n",
      "0 -1.245743      1.441459       -1.718118 -0.724791 -0.725224   \n",
      "1  0.113232      1.229569        0.162853 -0.725109 -0.725223   \n",
      "2 -0.455362      1.436386       -0.624143 -0.725362 -0.725264   \n",
      "3  1.165124      1.867372        1.618788 -0.725466 -0.725044   \n",
      "4  0.348258      1.721721        0.488155 -0.725460 -0.724928   \n",
      "\n",
      "   Volume_pct_change  Volume_MA5  Return_lag1  Return_lag2  Volatility_lag1  \n",
      "0          -0.649482   -0.615332    -0.161107     0.614987         0.420192  \n",
      "1          -0.266756   -0.616119    -1.566368    -0.161073         0.641917  \n",
      "2           0.276079   -0.616346     0.136393    -1.566493         0.623764  \n",
      "3           0.061046   -0.616502    -0.576041     0.136461         0.646338  \n",
      "4           0.529473   -0.616396     1.454387    -0.576053         0.618225   0    0.011782\n",
      "1    0.011681\n",
      "2    0.011807\n",
      "3    0.011650\n",
      "4    0.010676\n",
      "Name: Volatility, dtype: float64\n",
      "Train size: 14252, Test size: 3564\n"
     ]
    }
   ],
   "source": [
    "### Data Loading\n",
    "\n",
    "def load_csv(file_name: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load a CSV file from the data directory.\n",
    "    \"\"\"\n",
    "    data_path = Path.cwd().parent / \"data\" / file_name \n",
    "    if not data_path.exists():\n",
    "        raise FileNotFoundError(f\"{data_path} not found.\")\n",
    "\n",
    "    df = pd.read_csv(data_path, parse_dates=['Date'])\n",
    "    \n",
    "    # Parse 'Date' column explicitly\n",
    "    # MM/DD/YY format\n",
    "    df['Date'] = pd.to_datetime(df['Date'], format='%m/%d/%y', errors='coerce')\n",
    "    \n",
    "    # Correct future years (pandas may interpret '30' as 2030)\n",
    "    # Assume dates from 1928â€“2020\n",
    "    df.loc[df['Date'] > pd.Timestamp.today(), 'Date'] -= pd.offsets.DateOffset(years=100)\n",
    "    \n",
    "    # Sort by date\n",
    "    df = df.sort_values('Date').reset_index(drop=True)\n",
    "    return df \n",
    "\n",
    "\n",
    "\n",
    "def add_features(df: pd.DataFrame, rolling_window: int = 10):\n",
    "    \"\"\"\n",
    "    Add features for ML: returns, rolling volatility, price/volume features, lags.\n",
    "    Handles NaN and infinite values, ready for scaling.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Returns and volatility\n",
    "    df['Return'] = df['Adj Close'].pct_change()\n",
    "    df['Volatility'] = df['Return'].rolling(rolling_window).std()\n",
    "\n",
    "    # Price-based features\n",
    "    df['High_Low_pct'] = (df['High'] - df['Low']) / df['Close']\n",
    "    df['Close_Open_pct'] = (df['Close'] - df['Open']) / df['Open']\n",
    "    df['MA5'] = df['Close'].rolling(5).mean()\n",
    "    df['MA10'] = df['Close'].rolling(10).mean()\n",
    "\n",
    "    # Volume-based features\n",
    "    df['Volume_pct_change'] = df['Volume'].pct_change()\n",
    "    df['Volume_MA5'] = df['Volume'].rolling(5).mean()\n",
    "\n",
    "    # Lag features\n",
    "    df['Return_lag1'] = df['Return'].shift(1)\n",
    "    df['Return_lag2'] = df['Return'].shift(2)\n",
    "    df['Volatility_lag1'] = df['Volatility'].shift(1)\n",
    "\n",
    "    # Target: 10 days-ahead volatility\n",
    "    df['Volatility_future'] = df['Volatility'].shift(-10)\n",
    "\n",
    "    # Replace inf with NaN\n",
    "    df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "    # Fill volume-related NaNs (common in early data)\n",
    "    df['Volume_pct_change'] = df['Volume_pct_change'].fillna(0)\n",
    "    df['Volume_MA5'] = df['Volume_MA5'].ffill()\n",
    "\n",
    "    # Drop only rows where the key features are missing\n",
    "    df = df[df['Volatility'].notna() & df['Volatility_future'].notna()]\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # Features to scale\n",
    "    features = [\n",
    "        'Return', 'High_Low_pct', 'Close_Open_pct', 'MA5', 'MA10',\n",
    "        'Volume_pct_change', 'Volume_MA5', 'Return_lag1', 'Return_lag2', 'Volatility_lag1'\n",
    "    ]\n",
    "\n",
    "    # Scaling\n",
    "    scaler = StandardScaler()\n",
    "    df[features] = scaler.fit_transform(df[features].values)\n",
    "\n",
    "    target_col = 'Volatility_future'\n",
    "    return df, features, target_col\n",
    "\n",
    "\n",
    "# Load raw data\n",
    "df = load_csv(\"SPX.csv\")\n",
    "\n",
    "# Preprocess\n",
    "df_prepared, feature_cols, target_col = add_features(df)\n",
    "\n",
    "X = df_prepared[feature_cols]\n",
    "y = df_prepared[target_col]\n",
    "\n",
    "# Drop any row where X or y has NaN\n",
    "mask = (~X.isna().any(axis=1)) & (~y.isna())\n",
    "X = X[mask]\n",
    "y = y[mask]\n",
    "\n",
    "print(X.head(), y.head())\n",
    "\n",
    "\n",
    "# Split data into train (80%) and test (20%) by time\n",
    "split_idx = int(len(df_prepared) * 0.8)\n",
    "\n",
    "X_train, X_test = X.iloc[:split_idx], X.iloc[split_idx:]\n",
    "y_train, y_test = y.iloc[:split_idx], y.iloc[split_idx:]\n",
    "\n",
    "print(f\"Train size: {len(X_train)}, Test size: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9140cdad-a72a-48b1-9f80-f5e8dc7f99c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
