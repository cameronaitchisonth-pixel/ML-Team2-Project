{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5cb2a77f-a0c0-4603-8c91-2091c1edebf6",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe350561-04ba-4818-9cfd-c30a8d611a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Dependencies\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae0ec0da-1786-4bb6-af61-8a0c2c67cbf8",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bb460460-ab85-4639-8176-f9175d1cbf8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_891/1876270372.py:11: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df = pd.read_csv(data_path, parse_dates=['Date'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Inputs\n",
      "     Return  High_Low_pct  Close_Open_pct       MA5      MA10  \\\n",
      "1  0.023311      -0.92304       -0.023226 -0.647124 -0.647122   \n",
      "2 -0.217569      -0.92304       -0.023226 -0.647148 -0.647185   \n",
      "3  0.554534      -0.92304       -0.023226 -0.647173 -0.647209   \n",
      "4  0.454640      -0.92304       -0.023226 -0.647201 -0.647234   \n",
      "5  0.737950      -0.92304       -0.023226 -0.647104 -0.647214   \n",
      "\n",
      "   Volume_pct_change  Volume_MA5  Return_lag1  Return_lag2  Volatility_lag1  \n",
      "1          -0.060194   -0.522899    -1.399673     0.499832        -0.166255  \n",
      "2          -0.060194   -0.522899     0.023281    -1.399676        -0.217933  \n",
      "3          -0.060194   -0.522899    -0.217598     0.023273        -0.217969  \n",
      "4          -0.060194   -0.522899     0.554501    -0.217605        -0.203854  \n",
      "5          -0.060194   -0.522899     0.454607     0.554491        -0.210628  \n",
      "Data Outputs\n",
      "   Volatility_future  Volatility_future'  sign(Volatility_future')\n",
      "1           0.007275            0.000074                       1.0\n",
      "2           0.007273           -0.000002                      -1.0\n",
      "3           0.007181           -0.000092                      -1.0\n",
      "4           0.008303            0.001122                       1.0\n",
      "5           0.007715           -0.000588                      -1.0\n",
      "Train size: 18642, Test size: 4653\n"
     ]
    }
   ],
   "source": [
    "### Data Loading\n",
    "\n",
    "def load_csv(file_name: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load a CSV file from the data directory.\n",
    "    \"\"\"\n",
    "    data_path = Path.cwd().parent / \"data\" / file_name \n",
    "    if not data_path.exists():\n",
    "        raise FileNotFoundError(f\"{data_path} not found.\")\n",
    "\n",
    "    df = pd.read_csv(data_path, parse_dates=['Date'])\n",
    "    \n",
    "    # Parse 'Date' column explicitly\n",
    "    # MM/DD/YY format\n",
    "    df['Date'] = pd.to_datetime(df['Date'], format='%m/%d/%y', errors='coerce')\n",
    "    \n",
    "    # Correct future years (pandas may interpret '30' as 2030)\n",
    "    # Assume dates from 1928â€“2020\n",
    "    df.loc[df['Date'] > pd.Timestamp.today(), 'Date'] -= pd.offsets.DateOffset(years=100)\n",
    "    \n",
    "    # Sort by date\n",
    "    df = df.sort_values('Date').reset_index(drop=True)\n",
    "    return df \n",
    "\n",
    "\n",
    "\n",
    "def add_features(df: pd.DataFrame, rolling_window: int = 10):\n",
    "    \"\"\"\n",
    "    Add features for ML: returns, rolling volatility, price/volume features, lags.\n",
    "    Handles NaN and infinite values, ready for scaling.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Returns and volatility\n",
    "    df['Return'] = df['Adj Close'].pct_change()\n",
    "    df['Volatility'] = df['Return'].rolling(rolling_window).std()\n",
    "\n",
    "    # Price-based features\n",
    "    df['High_Low_pct'] = (df['High'] - df['Low']) / df['Close']\n",
    "    df['Close_Open_pct'] = (df['Close'] - df['Open']) / df['Open']\n",
    "    df['MA5'] = df['Close'].rolling(5).mean()\n",
    "    df['MA10'] = df['Close'].rolling(10).mean()\n",
    "\n",
    "    # Volume-based features\n",
    "    df['Volume_pct_change'] = df['Volume'].pct_change()\n",
    "    df['Volume_MA5'] = df['Volume'].rolling(5).mean()\n",
    "\n",
    "    # Lag features\n",
    "    df['Return_lag1'] = df['Return'].shift(1)\n",
    "    df['Return_lag2'] = df['Return'].shift(2)\n",
    "    df['Volatility_lag1'] = df['Volatility'].shift(1)\n",
    "\n",
    "    # Target: 10 days-ahead volatility\n",
    "    df['Volatility_future'] = df['Volatility'].shift(-10)\n",
    "\n",
    "    # Replace inf with NaN\n",
    "    df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "    # Fill volume-related NaNs (common in early data)\n",
    "    df['Volume_pct_change'] = df['Volume_pct_change'].fillna(0)\n",
    "    df['Volume_MA5'] = df['Volume_MA5'].ffill()\n",
    "\n",
    "    # Drop only rows where the key features are missing\n",
    "    df = df[df['Volatility'].notna() & df['Volatility_future'].notna()]\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # Features to scale\n",
    "    features = [\n",
    "        'Return', 'High_Low_pct', 'Close_Open_pct', 'MA5', 'MA10',\n",
    "        'Volume_pct_change', 'Volume_MA5', 'Return_lag1', 'Return_lag2', 'Volatility_lag1'\n",
    "    ]\n",
    "\n",
    "    # Scaling\n",
    "    scaler = StandardScaler()\n",
    "    df[features] = scaler.fit_transform(df[features].values)\n",
    "\n",
    "    target_col = 'Volatility_future'\n",
    "    return df, features, target_col\n",
    "\n",
    "\n",
    "# Load raw data\n",
    "df = load_csv(\"SPX.csv\")\n",
    "\n",
    "# Preprocess\n",
    "df_prepared, feature_cols, target_col = add_features(df)\n",
    "\n",
    "X = df_prepared[feature_cols]\n",
    "y = df_prepared[target_col]\n",
    "\n",
    "# Compute difference in volatility between samples\n",
    "y_prime = pd.Series(data=np.diff(y), index=pd.RangeIndex(start=1, stop=23303, step=1))\n",
    "y_prime_sign = pd.Series(data=y_prime / abs(y_prime), index=pd.RangeIndex(start=1, stop=23303, step=1), dtype=int)\n",
    "\n",
    "y = pd.DataFrame({\"Volatility_future\": y, \"Volatility_future'\": y_prime, \"sign(Volatility_future')\": y_prime_sign})\n",
    "\n",
    "# Drop any row where X or y has NaN\n",
    "mask = (~X.isna().any(axis=1)) & (~y.isna().any(axis=1))\n",
    "X = X[mask]\n",
    "y = y[mask]\n",
    "\n",
    "print(\"Data Inputs\")\n",
    "print(X.head())\n",
    "print(\"Data Outputs\")\n",
    "print(y.head())\n",
    "\n",
    "# _, (ax1, ax2) = plt.subplots(2, 1)\n",
    "# ax1.plot(y, label=\"y\")\n",
    "# ax1.plot(y_prime, label=\"y'\")\n",
    "# ax1.set_xlim([0, 10])\n",
    "# ax1.set_ylim([-0.001, 0.01])\n",
    "# ax1.grid()\n",
    "# ax1.legend()\n",
    "\n",
    "# ax2.plot(list(range(2, 23303)), y_prime_sign, label=\"abs(y')\")\n",
    "# ax2.set_xlim([0, 10])\n",
    "# ax2.grid()\n",
    "# ax2.legend()\n",
    "\n",
    "# Split data into train (80%) and test (20%) by time\n",
    "split_idx = int(len(df_prepared) * 0.8)\n",
    "\n",
    "X_train, X_test = X.iloc[:split_idx], X.iloc[split_idx:]\n",
    "y_train, y_test = y.iloc[:split_idx], y.iloc[split_idx:]\n",
    "\n",
    "print(f\"Train size: {len(X_train)}, Test size: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "961ce4db-443e-4138-9bad-161914c6f664",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1d73af8f-a1ce-49e8-92ec-0e01fc80cac3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 55.28%\n"
     ]
    }
   ],
   "source": [
    "# Import machine learning libraries\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Traing model\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train[\"sign(Volatility_future')\"])\n",
    "\n",
    "# Evaluate model\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test[\"sign(Volatility_future')\"], y_pred)\n",
    "print(f\"Accuracy = {round(100*accuracy,2)}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af801c5-1a8d-4e67-8b39-27db5a4caabc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12.3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
